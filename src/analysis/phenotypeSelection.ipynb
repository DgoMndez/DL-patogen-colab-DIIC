{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selección de Fenotipos para finetuning de BioBERT\n",
    "Domingo Méndez García. [domingo.mendezg@um.es](mailto:domingo.mendezg@um.es) [github.com/user/DgoMndez](https://github.com/DgoMndez)\n",
    "* Referencias:\n",
    "  * Modelo de partida: [pritamdeka](https://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb)\n",
    "  * Ontología: HPO versión https://github.com/obophenotype/human-phenotype-ontology/releases/tag/v2022-12-15\n",
    "\n",
    "TODO:\n",
    "* Breve sobre el problema y modelo que estamos tratando.\n",
    "* Resultados del finetuning anterior: ¿por qué cambiar los fenotipos?\n",
    "* Análisis de la ontología.\n",
    "  * IC y profundidad: las variables a tener en cuenta.\n",
    "  * Método de selección.\n",
    "  * Resultados.\n",
    "  * Decisión final informada.\n",
    "  \n",
    "## Problema: representación de fenotipos de HPO\n",
    "\n",
    "El modelo BERT es un Sentence-Transformer que mapea textos a vectores de 768 componentes que llamamos \"embeddings\", y puede adaptarse a diferentes tareas. Está especializado en textos científicos y médicos pero queremos fine-tunearlo usando un corpus de abstracts de PUBMED para mejorar su desempeño como etapa en PhenoLinker, un sistema que infiere relaciones entre genes y fenotipos para predecir patogenicidad. Los fenotipos que consideramos son los de la subontología Phenotypic Abnormality de HPO (que abreviamos HPO:PA). El objetivo entonces es conseguir que el embedding del nombre de un fenotipo represente mejor al fenotipo como nodo de la ontología. Para medir esto comparamos la similitud de los fenotipos en HPO (Lin) con la similitud coseno entre embeddings.\n",
    "\n",
    "## Evaluación del experimento de finetuning\n",
    "\n",
    "Notebook del experimento en [results-lprogress.ipynb](https://github.com/DgoMndez/DL-patogen-colab-DIIC/blob/main/src/fine-tuning/evaluation/results-lprogress.ipynb), con un resumen de cómo se ha entrenado al principio.\n",
    "\n",
    "* Medidas:\n",
    "\n",
    "![Pearson correlation vs Batches](./figures/pearson.png)\n",
    "\n",
    "![Spearman correlation vs Batches](./figures/spearman.png)\n",
    "\n",
    "![MSE vs Batches](./figures/spearman.png)\n",
    "\n",
    "* **Conclusiones**: Los mejores scores de evaluación se alcanzan a los pocos batches. Este finetuning no ha funcionado bien porque se alcanza el límite de aprendizaje muy pronto, pero no se percibe sobreajuste porque la tendencia del score train y test se parece.\n",
    "\n",
    "* **Justificación**:\n",
    "  * Distintas funciones de pérdida (BatchAllTripletLoss, CosineSimilarityLoss) o distintos hiperparámetros (lr, weight_decay, margin) pueden dar mejores resultados pero no creo que sea el factor determinante.\n",
    "  * La selección de fenotipos y el tamaño del índice de etiquetas es el factor determinante. Los nodos hoja no están bien representados en la ontología (casi todos tienen similitud lin ~ 0 seguramente porque no aparecen frecuentemente en la BD usada para calcular las similitudes). Esta es la principal explicación de los malos resultados del experimento: estos fenotipos hoja no son útiles para la evaluación y no representan bien HPO. Consecuentemente, hay que volver a obtener un corpus de abstracts con las búsquedas de los nuevos fenotipos y volver a preparar nuevos pares de evaluación y test.\n",
    "\n",
    "![Lin histogram](./figures/lin.png)\n",
    "\n",
    "Como vemos la gran mayoría de pares de fenotipos tanto de evaluación como de test tienen similitud 0. La similitud Lin se calcula a partir del IC de cada uno de los fenotipos y del de su ancestro común más profundo.\n",
    "\n",
    "![IC distribution](./figures/ic-dist-0.png) \n",
    "\n",
    "La distribución del IC de los fenotipos es bimodal por la gran cantidad de fenotipos con IC=0 (\"nulos\"). Estos fenotipos estimamos que causan problemas porque: la medida de similitud no es fiable (no hay ejemplos en la ontología para calcular el IC) y es muy probable que no se encuentren suficientes papers en PUBMED sobre ellos.\n",
    "\n",
    "\"The information content of each node in the HPO can be estimated through its frequency among annotations of the entire OMIM corpus.\" - [The Human Phenotype Ontology: A Tool for Annotating and Analyzing Human Hereditary Disease](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2668030/)\n",
    "\n",
    "## Análisis de la ontología\n",
    "\n",
    "A partir de los resultados anteriores queremos seleccionar un conjunto de fenotipos etiqueta que represente HPO:PA y nos sirva para entrenar, que llamaremos índice. El primer índice de fenotipos era una muestra de tamaño 100 de los nodos hoja de HPO:PA y no funcionó como deseábamos. Para obtener un mejor conjunto de etiquetas (fenotipos) hemos analizado HPO:PA para tomar una decisión.\n",
    "\n",
    "### IC y profundidad\n",
    "\n",
    "Tanto el IC como la profundidad son estimadores de la especifidad de un término de HPO. La profundidad se basa únicamente en la estructura de la ontología mientras que el IC se basa en la frecuencia del término (y sus hijos) en el corpus de referencia para la ontología. Por eso usaremos el IC medio de un índice como medida de bondad. Para seleccionar el índice \"cortamos\" el árbol a cierta profundidad $d$ y nos quedamos con los nodos hoja de ese subárbol:\n",
    "* Escogemos una profundidad $d$.\n",
    "* Seleccionamos todos los nodos hoja a profundidad menor que $d$.\n",
    "* Seleccionamos todos los nodos a profundidad $d$.\n",
    "* Quitamos todos los nodos nulos (IC=0).\n",
    "* El índice será una muestra de tamaño 2000 de los fenotipos que quedan. El tamaño viene determinado por la capacidad de búsqueda de abstracts en PUBMED y la potencia de cálculo para el finetuning. Antes habíamos entrenado con un tamaño 100 de índice y una CPU, tardando 8h, ahora hemos tomado un índice 20 veces mayor esperando que con GPU tengamos un finetuning mucho más rápido.\n",
    "\n",
    "### Resultados del análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>count</th>\n",
       "      <th>leafs</th>\n",
       "      <th>chosen</th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "      <th>trueCount</th>\n",
       "      <th>zeros</th>\n",
       "      <th>notZeros</th>\n",
       "      <th>meanGTZ</th>\n",
       "      <th>varGTZ</th>\n",
       "      <th>trueMean</th>\n",
       "      <th>trueVar</th>\n",
       "      <th>sampleCover</th>\n",
       "      <th>sampleCoverPercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6106</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1.177510</td>\n",
       "      <td>0.920597</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.177510</td>\n",
       "      <td>0.920597</td>\n",
       "      <td>1.177510</td>\n",
       "      <td>0.920597</td>\n",
       "      <td>6106</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>155</td>\n",
       "      <td>28</td>\n",
       "      <td>155</td>\n",
       "      <td>3.186360</td>\n",
       "      <td>5.403676</td>\n",
       "      <td>138.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>3.578882</td>\n",
       "      <td>4.659159</td>\n",
       "      <td>3.578882</td>\n",
       "      <td>4.659159</td>\n",
       "      <td>6106</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>318</td>\n",
       "      <td>828</td>\n",
       "      <td>4.368461</td>\n",
       "      <td>8.009591</td>\n",
       "      <td>680.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>5.279107</td>\n",
       "      <td>4.867119</td>\n",
       "      <td>5.308145</td>\n",
       "      <td>4.819274</td>\n",
       "      <td>6106</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2157</td>\n",
       "      <td>1198</td>\n",
       "      <td>2503</td>\n",
       "      <td>4.306913</td>\n",
       "      <td>9.162556</td>\n",
       "      <td>1846.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>5.727504</td>\n",
       "      <td>4.045136</td>\n",
       "      <td>5.876124</td>\n",
       "      <td>3.957335</td>\n",
       "      <td>6106</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3789</td>\n",
       "      <td>2502</td>\n",
       "      <td>5333</td>\n",
       "      <td>4.230784</td>\n",
       "      <td>10.081808</td>\n",
       "      <td>3610.0</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>2631.0</td>\n",
       "      <td>6.092907</td>\n",
       "      <td>3.170809</td>\n",
       "      <td>6.336812</td>\n",
       "      <td>2.997269</td>\n",
       "      <td>3835</td>\n",
       "      <td>62.807075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3696</td>\n",
       "      <td>2569</td>\n",
       "      <td>7742</td>\n",
       "      <td>4.321174</td>\n",
       "      <td>11.109246</td>\n",
       "      <td>4942.0</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>2469.0</td>\n",
       "      <td>6.468634</td>\n",
       "      <td>2.735594</td>\n",
       "      <td>6.688773</td>\n",
       "      <td>2.387720</td>\n",
       "      <td>2628</td>\n",
       "      <td>43.039633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2886</td>\n",
       "      <td>1985</td>\n",
       "      <td>9501</td>\n",
       "      <td>3.920541</td>\n",
       "      <td>12.640566</td>\n",
       "      <td>5688.0</td>\n",
       "      <td>1217.0</td>\n",
       "      <td>1669.0</td>\n",
       "      <td>6.779318</td>\n",
       "      <td>2.471154</td>\n",
       "      <td>6.905436</td>\n",
       "      <td>2.055930</td>\n",
       "      <td>2267</td>\n",
       "      <td>37.127416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1870</td>\n",
       "      <td>1535</td>\n",
       "      <td>10470</td>\n",
       "      <td>2.890284</td>\n",
       "      <td>12.384680</td>\n",
       "      <td>5916.0</td>\n",
       "      <td>1077.0</td>\n",
       "      <td>793.0</td>\n",
       "      <td>6.815676</td>\n",
       "      <td>2.437992</td>\n",
       "      <td>6.978093</td>\n",
       "      <td>1.920782</td>\n",
       "      <td>2092</td>\n",
       "      <td>34.261382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>678</td>\n",
       "      <td>543</td>\n",
       "      <td>10813</td>\n",
       "      <td>3.428010</td>\n",
       "      <td>12.456000</td>\n",
       "      <td>6049.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>6.678709</td>\n",
       "      <td>2.528724</td>\n",
       "      <td>7.003977</td>\n",
       "      <td>1.859949</td>\n",
       "      <td>2039</td>\n",
       "      <td>33.393384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>334</td>\n",
       "      <td>277</td>\n",
       "      <td>11012</td>\n",
       "      <td>3.247502</td>\n",
       "      <td>12.554429</td>\n",
       "      <td>6107.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>6.695467</td>\n",
       "      <td>2.737492</td>\n",
       "      <td>7.021452</td>\n",
       "      <td>1.836241</td>\n",
       "      <td>2004</td>\n",
       "      <td>32.820177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>106</td>\n",
       "      <td>95</td>\n",
       "      <td>11061</td>\n",
       "      <td>2.298786</td>\n",
       "      <td>11.704755</td>\n",
       "      <td>6109.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.166804</td>\n",
       "      <td>1.297056</td>\n",
       "      <td>7.028559</td>\n",
       "      <td>1.825223</td>\n",
       "      <td>2000</td>\n",
       "      <td>32.754668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>11077</td>\n",
       "      <td>1.613097</td>\n",
       "      <td>9.726571</td>\n",
       "      <td>6109.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.258935</td>\n",
       "      <td>1.398842</td>\n",
       "      <td>7.029655</td>\n",
       "      <td>1.824508</td>\n",
       "      <td>2000</td>\n",
       "      <td>32.754668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>11083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6106.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.029791</td>\n",
       "      <td>1.824605</td>\n",
       "      <td>2000</td>\n",
       "      <td>32.754668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6106.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.029791</td>\n",
       "      <td>1.824605</td>\n",
       "      <td>2000</td>\n",
       "      <td>32.754668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    depth  count  leafs  chosen      mean        var  trueCount   zeros  \\\n",
       "0       0      1      0       1  0.000817        NaN        1.0     0.0   \n",
       "1       1     23      0      23  1.177510   0.920597       23.0     0.0   \n",
       "2       2    155     28     155  3.186360   5.403676      138.0    17.0   \n",
       "3       3    800    318     828  4.368461   8.009591      680.0   138.0   \n",
       "4       4   2157   1198    2503  4.306913   9.162556     1846.0   535.0   \n",
       "5       5   3789   2502    5333  4.230784  10.081808     3610.0  1158.0   \n",
       "6       6   3696   2569    7742  4.321174  11.109246     4942.0  1227.0   \n",
       "7       7   2886   1985    9501  3.920541  12.640566     5688.0  1217.0   \n",
       "8       8   1870   1535   10470  2.890284  12.384680     5916.0  1077.0   \n",
       "9       9    678    543   10813  3.428010  12.456000     6049.0   330.0   \n",
       "10     10    334    277   11012  3.247502  12.554429     6107.0   172.0   \n",
       "11     11    106     95   11061  2.298786  11.704755     6109.0    72.0   \n",
       "12     12     27     20   11077  1.613097   9.726571     6109.0    21.0   \n",
       "13     13     13     12   11083  0.000000   0.000000     6106.0    13.0   \n",
       "14     14      2      2   11084  0.000000   0.000000     6106.0     2.0   \n",
       "\n",
       "    notZeros   meanGTZ    varGTZ  trueMean   trueVar  sampleCover  \\\n",
       "0        1.0  0.000817       NaN  0.000817       NaN         6106   \n",
       "1       23.0  1.177510  0.920597  1.177510  0.920597         6106   \n",
       "2      138.0  3.578882  4.659159  3.578882  4.659159         6106   \n",
       "3      662.0  5.279107  4.867119  5.308145  4.819274         6106   \n",
       "4     1622.0  5.727504  4.045136  5.876124  3.957335         6106   \n",
       "5     2631.0  6.092907  3.170809  6.336812  2.997269         3835   \n",
       "6     2469.0  6.468634  2.735594  6.688773  2.387720         2628   \n",
       "7     1669.0  6.779318  2.471154  6.905436  2.055930         2267   \n",
       "8      793.0  6.815676  2.437992  6.978093  1.920782         2092   \n",
       "9      348.0  6.678709  2.528724  7.003977  1.859949         2039   \n",
       "10     162.0  6.695467  2.737492  7.021452  1.836241         2004   \n",
       "11      34.0  7.166804  1.297056  7.028559  1.825223         2000   \n",
       "12       6.0  7.258935  1.398842  7.029655  1.824508         2000   \n",
       "13       0.0       NaN       NaN  7.029791  1.824605         2000   \n",
       "14       0.0       NaN       NaN  7.029791  1.824605         2000   \n",
       "\n",
       "    sampleCoverPercent  \n",
       "0           100.000000  \n",
       "1           100.000000  \n",
       "2           100.000000  \n",
       "3           100.000000  \n",
       "4           100.000000  \n",
       "5            62.807075  \n",
       "6            43.039633  \n",
       "7            37.127416  \n",
       "8            34.261382  \n",
       "9            33.393384  \n",
       "10           32.820177  \n",
       "11           32.754668  \n",
       "12           32.754668  \n",
       "13           32.754668  \n",
       "14           32.754668  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dfDepth = pd.read_csv('results/depth_count.csv', sep='\\t')\n",
    "columnsDrop = ['subtree', 'subtreePercent', 'fullCover', 'sample', 'out']\n",
    "display(dfDepth.drop(columns=columnsDrop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decisión**: tomar la selección (sin nulos) a profundidad $d=10$ porque:\n",
    "* El IC medio de la selección crece con la profundidad, pero se estanca a esa profundidad 10 en las centésimas, aparte de que a partir de esa profundidad se añaden muy pocos nodos no nulos.\n",
    "* Con una muestra de tamaño 2000 de la selección a profundidad 10 cubrimos un 32.82% de los nodos hoja no nulos de la ontología, que es un porcentaje suficiente.\n",
    "* Importante considerar que si hacemos la muestra completa de 6106 nodos no nulos cubrimos el 100%.\n",
    "* La clave ha sido quitar los nulos, lo que ha dado unos valores de IC medio y sampleCoverPercent aceptables a partir de $d=5$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
