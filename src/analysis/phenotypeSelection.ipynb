{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selección de Fenotipos para finetuning de BioBERT\n",
    "Domingo Méndez García. [domingo.mendezg@um.es](mailto:domingo.mendezg@um.es) [github.com/user/DgoMndez](https://github.com/DgoMndez)\n",
    "* Referencias:\n",
    "  * Modelo de partida: [pritamdeka](https://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb)\n",
    "  * Ontología: HPO versión https://github.com/obophenotype/human-phenotype-ontology/releases/tag/v2022-12-15\n",
    "\n",
    "TODO:\n",
    "* Breve sobre el problema y modelo que estamos tratando.\n",
    "* Resultados del finetuning anterior: ¿por qué cambiar los fenotipos?\n",
    "* Análisis de la ontología.\n",
    "  * IC y profundidad: las variables a tener en cuenta.\n",
    "  * Método de selección.\n",
    "  * Resultados.\n",
    "  * Decisión final informada.\n",
    "  \n",
    "## Problema: representación de fenotipos de HPO\n",
    "\n",
    "El modelo BERT es un Sentence-Transformer que mapea textos a vectores de 768 componentes que llamamos \"embeddings\", y puede adaptarse a diferentes tareas. Está especializado en textos científicos y médicos pero queremos fine-tunearlo usando un corpus de abstracts de PUBMED para mejorar su desempeño como etapa en PhenoLinker, un sistema que infiere relaciones entre genes y fenotipos para predecir patogenicidad. Los fenotipos que consideramos son los de la subontología Phenotypic Abnormality de HPO (que abreviamos HPO:PA). El objetivo entonces es conseguir que el embedding del nombre de un fenotipo represente mejor al fenotipo como nodo de la ontología. Para medir esto comparamos la similitud de los fenotipos en HPO (Lin) con la similitud coseno entre embeddings.\n",
    "\n",
    "## Evaluación del experimento de finetuning\n",
    "\n",
    "Notebook del experimento en [results-lprogress.ipynb](https://github.com/DgoMndez/DL-patogen-colab-DIIC/blob/main/src/fine-tuning/evaluation/results-lprogress.ipynb), con un resumen de cómo se ha entrenado al principio.\n",
    "\n",
    "### Datos:\n",
    "* **Ejemplos de entrenamiento:** pares (abstract, fenotipo).\n",
    "* **Ejemplos de evaluación:** (fenotipo1, fenotipo2, gold), donde gold es la similitud Lin entre los fenotipos y se compara con la similitud coseno entre embeddings.\n",
    "\n",
    "En concreto:\n",
    "\n",
    "* Muestra fenotipos/etiquetas: [index.csv](../../data/phenotypes/index.csv)\n",
    "  * $M = 100$ tags o fenotipos de entrenamiento. \n",
    "  * Todos los pares de tags fueron usados para la evaluación de training.\n",
    "* Abstracts de los que se tomó la muestra: [abstracts.csv](../../data/abstracts/abstracts.csv)\n",
    "  * $N = 11613$ abstracts.\n",
    "  * Obtenidos de una búsqueda en [pubmed](https://pubmed.ncbi.nlm.nih.gov/)\n",
    "* Fenotipos test: [leaf-phenotypes.csv](../../data/phenotypes/leaf-phenotypes.csv) = nodos hoja HPO:PhenotypicAbnormality.\n",
    "  * Muestra de 1000 pares de fenotipos para la evaluación de test [pairs-test.csv](../data/evaluation/pairs-test.csv).\n",
    "## Resumen del modelo:\n",
    "Modelo original: [pritamdeka](https://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb).  \n",
    "Modelo obtenido: [README.md](../../output/fine-tuned-bio-bert-ev-mse-01-04-2024/README.md)  \n",
    "\n",
    "* Medidas:\n",
    "\n",
    "![Pearson correlation vs Batches](./figures/pearson.png)\n",
    "\n",
    "![Spearman correlation vs Batches](./figures/spearman.png)\n",
    "\n",
    "![MSE vs Batches](./figures/spearman.png)\n",
    "\n",
    "* **Conclusiones**: Los mejores scores de evaluación se alcanzan a los pocos batches. Este finetuning no ha funcionado bien porque se alcanza el límite de aprendizaje muy pronto, pero no se percibe sobreajuste porque la tendencia del score train y test se parece.\n",
    "\n",
    "* **Justificación**:\n",
    "  * Distintas funciones de pérdida (BatchAllTripletLoss, CosineSimilarityLoss) o distintos hiperparámetros (lr, weight_decay, margin) pueden dar mejores resultados pero no creo que sea el factor determinante.\n",
    "  * La selección de fenotipos y el tamaño del índice de etiquetas es el factor determinante. Los nodos hoja no están bien representados en la ontología (casi todos tienen similitud lin ~ 0 seguramente porque no aparecen frecuentemente en la BD usada para calcular las similitudes). Esta es la principal explicación de los malos resultados del experimento: estos fenotipos hoja no son útiles para la evaluación y no representan bien HPO. Consecuentemente, hay que volver a obtener un corpus de abstracts con las búsquedas de los nuevos fenotipos y volver a preparar nuevos pares de evaluación y test.\n",
    "\n",
    "![Lin histogram](./figures/lin.png)\n",
    "\n",
    "Como vemos la gran mayoría de pares de fenotipos tanto de evaluación como de test tienen similitud 0. La similitud Lin se calcula a partir del IC de cada uno de los fenotipos y del de su ancestro común más profundo.\n",
    "\n",
    "![IC distribution](./figures/ic-dist-0.png) \n",
    "\n",
    "La distribución del IC de los fenotipos es bimodal por la gran cantidad de fenotipos con IC=0 (\"nulos\"). Estos fenotipos estimamos que causan problemas porque: la medida de similitud no es fiable (no hay ejemplos en la ontología para calcular el IC) y es muy probable que no se encuentren suficientes papers en PUBMED sobre ellos.\n",
    "\n",
    "\"The information content of each node in the HPO can be estimated through its frequency among annotations of the entire OMIM corpus.\" - [The Human Phenotype Ontology: A Tool for Annotating and Analyzing Human Hereditary Disease](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2668030/)\n",
    "\n",
    "## Análisis de la ontología\n",
    "\n",
    "A partir de los resultados anteriores queremos seleccionar un conjunto de fenotipos etiqueta que represente HPO:PA y nos sirva para entrenar, que llamaremos índice. El primer índice de fenotipos era una muestra de tamaño 100 de los nodos hoja de HPO:PA y no funcionó como deseábamos. Para obtener un mejor conjunto de etiquetas (fenotipos) hemos analizado HPO:PA para tomar una decisión.\n",
    "\n",
    "### IC y profundidad\n",
    "\n",
    "<figure>\n",
    "  <img src=\"./figures/ic-hist-full.png\" alt=\"IC-hist-full\">\n",
    "  <figcaption>Histograma de IC para HPO:PA completa</figcaption>\n",
    "</figure>\n",
    "\n",
    "El histograma de IC de HPO:PA muestra la gran cantidad de fenotipos nulos que hay.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"./figures/meandepth.png\" alt=\"meandepth\">\n",
    "  <figcaption>Profundidad media vs Porcentaje de nodos</figcaption>\n",
    "</figure>\n",
    "\n",
    "En el eje X tenemos porcentajes de muestra. La profundidad media en Y quiere decir la profundidad media del X% de nodos menos profundos de la ontología. Esta idea de selección fue descartada por la siguiente.\n",
    "\n",
    "Tanto el IC como la profundidad son estimadores de la especifidad de un término de HPO. La profundidad se basa únicamente en la estructura de la ontología mientras que el IC se basa en la frecuencia del término (y sus hijos) en el corpus de referencia para la ontología. Por eso usaremos el IC medio de un índice como medida de bondad. Para seleccionar el índice \"cortamos\" el árbol a cierta profundidad $d$ y nos quedamos con los nodos hoja de ese subárbol:\n",
    "<a name=\"selection-method\"></a>\n",
    "* Escogemos una profundidad $d$.\n",
    "* Seleccionamos todos los nodos hoja a profundidad menor que $d$.\n",
    "* Seleccionamos todos los nodos a profundidad $d$.\n",
    "  * Quitamos todos los nodos nulos (IC=0). Después del análisis añadimos este paso.\n",
    "* El índice será una muestra de tamaño 2000 de los fenotipos que quedan. El tamaño viene determinado por la capacidad de búsqueda de abstracts en PUBMED y la potencia de cálculo para el finetuning. Antes habíamos entrenado con un tamaño 100 de índice y una CPU, tardando 8h, ahora hemos tomado un índice 20 veces mayor esperando que con GPU tengamos un finetuning mucho más rápido.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"./figures/tags-vs-depth.png\" alt=\"tags_vs_depth\">\n",
    "  <figcaption>Número de etiquetas del índice vs Profundidad</figcaption>\n",
    "</figure>\n",
    "\n",
    "La gráfica muestra el número de etiquetas de índice del corte (contando los nulos), lo que nos dio la primera idea del tamaño de muestra a cada profundidad.\n",
    "\n",
    "### Resultados del análisis\n",
    "Para realizar el análisis se tomaron varias medidas a cada profundidad con el objetivo de escoger la más apropiada.\n",
    "\n",
    "1. **Contar**\n",
    "\n",
    "Lo primero fue contar el número de nodos (count), nodos hoja (leafs), nodos nulos (zeros), media y cuasivarianza de IC (mean, var) y tamaño de la selección (chosen) a cada profundidad. En este primer conteo no se habían eliminado los nulos de la selección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>count</th>\n",
       "      <th>leafs</th>\n",
       "      <th>zeros</th>\n",
       "      <th>chosen</th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>1.177510</td>\n",
       "      <td>0.920597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>155</td>\n",
       "      <td>28</td>\n",
       "      <td>17.0</td>\n",
       "      <td>155</td>\n",
       "      <td>3.186360</td>\n",
       "      <td>5.403676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>318</td>\n",
       "      <td>138.0</td>\n",
       "      <td>828</td>\n",
       "      <td>4.368461</td>\n",
       "      <td>8.009591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2157</td>\n",
       "      <td>1198</td>\n",
       "      <td>535.0</td>\n",
       "      <td>2503</td>\n",
       "      <td>4.306913</td>\n",
       "      <td>9.162556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3789</td>\n",
       "      <td>2502</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>5333</td>\n",
       "      <td>4.230784</td>\n",
       "      <td>10.081808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3696</td>\n",
       "      <td>2569</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>7742</td>\n",
       "      <td>4.321174</td>\n",
       "      <td>11.109246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2886</td>\n",
       "      <td>1985</td>\n",
       "      <td>1217.0</td>\n",
       "      <td>9501</td>\n",
       "      <td>3.920541</td>\n",
       "      <td>12.640566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1870</td>\n",
       "      <td>1535</td>\n",
       "      <td>1077.0</td>\n",
       "      <td>10470</td>\n",
       "      <td>2.890284</td>\n",
       "      <td>12.384680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>678</td>\n",
       "      <td>543</td>\n",
       "      <td>330.0</td>\n",
       "      <td>10813</td>\n",
       "      <td>3.428010</td>\n",
       "      <td>12.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>334</td>\n",
       "      <td>277</td>\n",
       "      <td>172.0</td>\n",
       "      <td>11012</td>\n",
       "      <td>3.247502</td>\n",
       "      <td>12.554429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>106</td>\n",
       "      <td>95</td>\n",
       "      <td>72.0</td>\n",
       "      <td>11061</td>\n",
       "      <td>2.298786</td>\n",
       "      <td>11.704755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11077</td>\n",
       "      <td>1.613097</td>\n",
       "      <td>9.726571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    depth  count  leafs   zeros  chosen      mean        var\n",
       "0       0      1      0     0.0       1  0.000817        NaN\n",
       "1       1     23      0     0.0      23  1.177510   0.920597\n",
       "2       2    155     28    17.0     155  3.186360   5.403676\n",
       "3       3    800    318   138.0     828  4.368461   8.009591\n",
       "4       4   2157   1198   535.0    2503  4.306913   9.162556\n",
       "5       5   3789   2502  1158.0    5333  4.230784  10.081808\n",
       "6       6   3696   2569  1227.0    7742  4.321174  11.109246\n",
       "7       7   2886   1985  1217.0    9501  3.920541  12.640566\n",
       "8       8   1870   1535  1077.0   10470  2.890284  12.384680\n",
       "9       9    678    543   330.0   10813  3.428010  12.456000\n",
       "10     10    334    277   172.0   11012  3.247502  12.554429\n",
       "11     11    106     95    72.0   11061  2.298786  11.704755\n",
       "12     12     27     20    21.0   11077  1.613097   9.726571\n",
       "13     13     13     12    13.0   11083  0.000000   0.000000\n",
       "14     14      2      2     2.0   11084  0.000000   0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dfDepth = pd.read_csv('results/depth_count.csv', sep='\\t')\n",
    "columns = ['depth', 'count', 'leafs', 'zeros', 'chosen', 'mean', 'var']\n",
    "display(dfDepth[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que vi fue que el IC medio aumentaba hasta profundidad 6 y después disminuía, y por la alta varianza especialmente a profundidades mayores me di cuenta de que los nulos y el carácter bimodal del IC tenían mucho que ver. Al principio pensaba que solo había tantos nulos a profundidades mayores, que son las que pensaba que tenían nodos más específicos. Al ver que había muchos nodos nulos en todas las profundidades decidí quitarlos y repetir los análisis de IC medio y varianza sin ellos.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"./results/images/ic_depth_5.png\" alt=\"hist d=5\">\n",
    "  <figcaption>Histograma de IC a profundidad 5</figcaption>\n",
    "</figure>\n",
    "\n",
    "En cualquier histograma a cualquier profundidad notamos una gran cantidad de nulos.\n",
    "\n",
    "2. **Nulos vs no-nulos**\n",
    "\n",
    "Ahora repetía el conteo midiendo por separado el IC medio (meanGTZ) y varianza (varGTZ) de los no nulos a cada profundidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>count</th>\n",
       "      <th>chosen</th>\n",
       "      <th>zeros</th>\n",
       "      <th>notZeros</th>\n",
       "      <th>meanGTZ</th>\n",
       "      <th>varGTZ</th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.177510</td>\n",
       "      <td>0.920597</td>\n",
       "      <td>1.177510</td>\n",
       "      <td>0.920597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>17.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>3.578882</td>\n",
       "      <td>4.659159</td>\n",
       "      <td>3.186360</td>\n",
       "      <td>5.403676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>828</td>\n",
       "      <td>138.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>5.279107</td>\n",
       "      <td>4.867119</td>\n",
       "      <td>4.368461</td>\n",
       "      <td>8.009591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2157</td>\n",
       "      <td>2503</td>\n",
       "      <td>535.0</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>5.727504</td>\n",
       "      <td>4.045136</td>\n",
       "      <td>4.306913</td>\n",
       "      <td>9.162556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3789</td>\n",
       "      <td>5333</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>2631.0</td>\n",
       "      <td>6.092907</td>\n",
       "      <td>3.170809</td>\n",
       "      <td>4.230784</td>\n",
       "      <td>10.081808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3696</td>\n",
       "      <td>7742</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>2469.0</td>\n",
       "      <td>6.468634</td>\n",
       "      <td>2.735594</td>\n",
       "      <td>4.321174</td>\n",
       "      <td>11.109246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2886</td>\n",
       "      <td>9501</td>\n",
       "      <td>1217.0</td>\n",
       "      <td>1669.0</td>\n",
       "      <td>6.779318</td>\n",
       "      <td>2.471154</td>\n",
       "      <td>3.920541</td>\n",
       "      <td>12.640566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1870</td>\n",
       "      <td>10470</td>\n",
       "      <td>1077.0</td>\n",
       "      <td>793.0</td>\n",
       "      <td>6.815676</td>\n",
       "      <td>2.437992</td>\n",
       "      <td>2.890284</td>\n",
       "      <td>12.384680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>678</td>\n",
       "      <td>10813</td>\n",
       "      <td>330.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>6.678709</td>\n",
       "      <td>2.528724</td>\n",
       "      <td>3.428010</td>\n",
       "      <td>12.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>334</td>\n",
       "      <td>11012</td>\n",
       "      <td>172.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>6.695467</td>\n",
       "      <td>2.737492</td>\n",
       "      <td>3.247502</td>\n",
       "      <td>12.554429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>106</td>\n",
       "      <td>11061</td>\n",
       "      <td>72.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.166804</td>\n",
       "      <td>1.297056</td>\n",
       "      <td>2.298786</td>\n",
       "      <td>11.704755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>11077</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.258935</td>\n",
       "      <td>1.398842</td>\n",
       "      <td>1.613097</td>\n",
       "      <td>9.726571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>11083</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>11084</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    depth  count  chosen   zeros  notZeros   meanGTZ    varGTZ      mean  \\\n",
       "0       0      1       1     0.0       1.0  0.000817       NaN  0.000817   \n",
       "1       1     23      23     0.0      23.0  1.177510  0.920597  1.177510   \n",
       "2       2    155     155    17.0     138.0  3.578882  4.659159  3.186360   \n",
       "3       3    800     828   138.0     662.0  5.279107  4.867119  4.368461   \n",
       "4       4   2157    2503   535.0    1622.0  5.727504  4.045136  4.306913   \n",
       "5       5   3789    5333  1158.0    2631.0  6.092907  3.170809  4.230784   \n",
       "6       6   3696    7742  1227.0    2469.0  6.468634  2.735594  4.321174   \n",
       "7       7   2886    9501  1217.0    1669.0  6.779318  2.471154  3.920541   \n",
       "8       8   1870   10470  1077.0     793.0  6.815676  2.437992  2.890284   \n",
       "9       9    678   10813   330.0     348.0  6.678709  2.528724  3.428010   \n",
       "10     10    334   11012   172.0     162.0  6.695467  2.737492  3.247502   \n",
       "11     11    106   11061    72.0      34.0  7.166804  1.297056  2.298786   \n",
       "12     12     27   11077    21.0       6.0  7.258935  1.398842  1.613097   \n",
       "13     13     13   11083    13.0       0.0       NaN       NaN  0.000000   \n",
       "14     14      2   11084     2.0       0.0       NaN       NaN  0.000000   \n",
       "\n",
       "          var  \n",
       "0         NaN  \n",
       "1    0.920597  \n",
       "2    5.403676  \n",
       "3    8.009591  \n",
       "4    9.162556  \n",
       "5   10.081808  \n",
       "6   11.109246  \n",
       "7   12.640566  \n",
       "8   12.384680  \n",
       "9   12.456000  \n",
       "10  12.554429  \n",
       "11  11.704755  \n",
       "12   9.726571  \n",
       "13   0.000000  \n",
       "14   0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = ['depth', 'count', 'chosen', 'zeros', 'notZeros', 'meanGTZ', 'varGTZ', 'mean', 'var']\n",
    "display(dfDepth[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que vi fue que la varianza se reducía considerablemente y ahora el IC medio era creciente con la profundidad (salvo profundidades 13 y 14 que solo tienen nulos). Así que a partir de aquí la hipótesis era utilizar una profundidad de 10, que es muy alta y a partir de ahí se añaden muy pocos no nulos.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"./results/images/ic_depth_10_not_zeros.png\" alt=\"hist IC>0 d=10\">\n",
    "  <figcaption>Histograma de IC>0 a profundidad 10</figcaption>\n",
    "</figure>\n",
    "\n",
    "Ahora al  quitar los nulos en cualquier profundidad (por ejemplo 10) vemos que predominan los nodos con mayor IC en la selección y esperamos que esto signifique una similitud Lin más fiable entre pares de fenotipos.\n",
    "\n",
    "Pero antes de tomar la decisión final debía repetir los conteos en lugar de considerando la profundidad $d$ justa, considerando la selección (añadiendo en cada fila los nodos hoja de profundidad $<d$). También debía añadir el % cubierto de ontología con una muestra de tamaño 2000, para garantizar que la muestra es suficientemente grande como para representar la ontología.\n",
    "\n",
    "3. **Selección y muestra final**\n",
    "\n",
    "Se realizaron los conteos y medidas del IC para la selección especificada en [IC y profundidad](#ic-y-profundidad) y finalmente se calculó para cada profundiad el % de ontología cubierto por una muestra aleatoria de tamaño 2000.\n",
    "\n",
    "El % de ontología cubierto $cub(F)$ de un conjunto de fenotipos $F$ se define de la siguiente manera:\n",
    "* Tomar $S$ el conjunto de nodos hoja alcanzables desde algún nodo de $F$ por las aristas de relación padre-hijo de la ontología.\n",
    "* Tomar $S^*$ el subconjunto de los nodos no nulos de $S$.\n",
    "* $cub(F) = 100|S|/H^*\\%$ donde $H^*$ es el conjunto de nodos hoja no nulos de HPO:PA.\n",
    "\n",
    "Ahora trueCount, trueMean y trueVar son el número de nodos, la media de IC y la cuasivarianza para la selección especificada a profundidad $d$. sample  es el tamaño de muestra y sampleCoverPercent el % de ontología cubierto por la muestra.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"./results/images/ic_depth_10_true.png\" alt=\"hist d=10\">\n",
    "  <figcaption>Histograma de IC de la selección a profundidad 10</figcaption>\n",
    "</figure>\n",
    "\n",
    "La distribución del IC de la selección es similar a la de profundidad 10, los nodos con más información son más frecuentes, pero en este caso hemos considerado toda la selección de los 6107 nodos no nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>trueCount</th>\n",
       "      <th>trueMean</th>\n",
       "      <th>trueVar</th>\n",
       "      <th>sampleCoverPercent</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.177510</td>\n",
       "      <td>0.920597</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>138.0</td>\n",
       "      <td>3.578882</td>\n",
       "      <td>4.659159</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>680.0</td>\n",
       "      <td>5.308145</td>\n",
       "      <td>4.819274</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1846.0</td>\n",
       "      <td>5.876124</td>\n",
       "      <td>3.957335</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3610.0</td>\n",
       "      <td>6.336812</td>\n",
       "      <td>2.997269</td>\n",
       "      <td>62.807075</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>4942.0</td>\n",
       "      <td>6.688773</td>\n",
       "      <td>2.387720</td>\n",
       "      <td>43.039633</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5688.0</td>\n",
       "      <td>6.905436</td>\n",
       "      <td>2.055930</td>\n",
       "      <td>37.127416</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5916.0</td>\n",
       "      <td>6.978093</td>\n",
       "      <td>1.920782</td>\n",
       "      <td>34.261382</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>6049.0</td>\n",
       "      <td>7.003977</td>\n",
       "      <td>1.859949</td>\n",
       "      <td>33.393384</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>6107.0</td>\n",
       "      <td>7.021452</td>\n",
       "      <td>1.836241</td>\n",
       "      <td>32.820177</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>6109.0</td>\n",
       "      <td>7.028559</td>\n",
       "      <td>1.825223</td>\n",
       "      <td>32.754668</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>6109.0</td>\n",
       "      <td>7.029655</td>\n",
       "      <td>1.824508</td>\n",
       "      <td>32.754668</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>6106.0</td>\n",
       "      <td>7.029791</td>\n",
       "      <td>1.824605</td>\n",
       "      <td>32.754668</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>6106.0</td>\n",
       "      <td>7.029791</td>\n",
       "      <td>1.824605</td>\n",
       "      <td>32.754668</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    depth  trueCount  trueMean   trueVar  sampleCoverPercent  sample\n",
       "0       0        1.0  0.000817       NaN          100.000000       1\n",
       "1       1       23.0  1.177510  0.920597          100.000000      23\n",
       "2       2      138.0  3.578882  4.659159          100.000000     138\n",
       "3       3      680.0  5.308145  4.819274          100.000000     680\n",
       "4       4     1846.0  5.876124  3.957335          100.000000    1846\n",
       "5       5     3610.0  6.336812  2.997269           62.807075    2000\n",
       "6       6     4942.0  6.688773  2.387720           43.039633    2000\n",
       "7       7     5688.0  6.905436  2.055930           37.127416    2000\n",
       "8       8     5916.0  6.978093  1.920782           34.261382    2000\n",
       "9       9     6049.0  7.003977  1.859949           33.393384    2000\n",
       "10     10     6107.0  7.021452  1.836241           32.820177    2000\n",
       "11     11     6109.0  7.028559  1.825223           32.754668    2000\n",
       "12     12     6109.0  7.029655  1.824508           32.754668    2000\n",
       "13     13     6106.0  7.029791  1.824605           32.754668    2000\n",
       "14     14     6106.0  7.029791  1.824605           32.754668    2000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = ['depth', 'trueCount', 'trueMean', 'trueVar', 'sampleCoverPercent', 'sample']\n",
    "display(dfDepth[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"./figures/samplecover.png\" alt=\"samplecover\">\n",
    "  <figcaption>% Cubierto de H* vs profundidad</figcaption>\n",
    "</figure>\n",
    "\n",
    "En la tabla vemos que se repite la tendencia creciente del IC con la profundidad y que el % cubierto de la ontología decrece con la profundidad hasta el 32.75%. Con esto se toma la decisión final de a qué profundidad tomar la selección y si la muestra de tamaño 2000 es válida.\n",
    "\n",
    "**Decisión**: tomar la selección (sin nulos) a profundidad $d=10$ porque:\n",
    "* El IC medio de la selección crece con la profundidad, pero se estanca a esa profundidad 10 en las centésimas, aparte de que a partir de esa profundidad se añaden muy pocos nodos no nulos.\n",
    "* Con una muestra de tamaño 2000 de la selección a profundidad 10 cubrimos un 32.82% de los nodos hoja no nulos de la ontología, que es un porcentaje suficiente.\n",
    "* Importante considerar que si hacemos la muestra completa de 6106 nodos no nulos cubrimos el 100%.\n",
    "* La clave ha sido quitar los nulos, lo que ha dado unos valores de IC medio y sampleCoverPercent aceptables a partir de $d=5$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
