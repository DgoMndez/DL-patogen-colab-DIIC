{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer intento de evaluación comparando Lin y Coseno\n",
    "El modelo se ha finetuneado con un total de 15392 abstracts (962 batches de 16) obtenidos de consultas a pubmed de 100 fenotipos. Los datos están en:\n",
    "* Abstracts de los que se tomó la muestra: [abstracts.csv](../pubmed-queries/abstracts/abstracts.csv)\n",
    "* Fenotipos/etiquetas: [index-phenotypes.csv](../pubmed-queries/abstracts/index-phenotypes.csv)\n",
    "* Modelo final: [./output/fine-tuned-bio-bert-cosbatch](./output/fine-tuned-bio-bert-cosbatch/README.md)  \n",
    "No se usó GPU, no recuerdo el tiempo de ejecución (<4h). La función de pérdida ha sido BatchAllTripletLoss con distancia de coseno y\n",
    "margin=0.3743 # mínima distancia coseno entre los embeddings originales de una muestra aleatoria de pares de fenotipos del index.  \n",
    "El proceso de evaluación fue:\n",
    "* Distancias comparadas: 1-cos, 1-lin: $$delta(x,y) = (1-cos(x,y)) - (1-lin(x,y))$$\n",
    "* Medidas obtenidas: MSE y correlación\n",
    "* Modelos evaluados: Original y tuneado\n",
    "* Tipos de error:\n",
    "  * Error de ajuste (Train):\n",
    "    * Datos: pares de fenotipos del index (todas las combinaciones)\n",
    "  * Error de test:\n",
    "    * Datos: muestra de tamaño 1000 de pares de fenotipos ([phenotypes-22-12-15.csv](../pubmed-queries/results/phenotypes-22-12-15.csv) = nodos hoja HPO:PhenotypicAbnormality)\n",
    "Resultados en [evaluation.txt](../pubmed-queries/abstracts/evaluation.txt), [pairs-deltas.csv](../pubmed-queries/abstracts/pairs-deltas.csv) (train) y [test-pairs.csv](../pubmed-queries/abstracts/test-pairs.csv) (test).\n",
    "# 1. Cargar todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/domingo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/domingo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tags\n",
      "0                    Temporomandibular joint ankylosis\n",
      "1                                             Dyslexia\n",
      "2    Stippling of the epiphysis of the proximal pha...\n",
      "3                                 Ankle joint effusion\n",
      "4                              Reduced C-peptide level\n",
      "Name: phenotypeName, dtype: object\n",
      "Na's: 1854\n",
      "Train :  1710 \n",
      "\n",
      "paperId                                                   18789087\n",
      "phenotypeId                                             HP:0032217\n",
      "phenotypeName                                     Indurated nodule\n",
      "title            Congenital atrophic dermatofibrosarcoma protub...\n",
      "abstract         Dermatofibrosarcoma protuberans is a rare, mal...\n",
      "Name: 4591, dtype: object\n",
      "paperId                                                   11999861\n",
      "phenotypeId                                             HP:0030553\n",
      "phenotypeName                    Visual acuity no light perception\n",
      "title                            Visual marking and visual change.\n",
      "abstract         Five experiments investigated the types of cha...\n",
      "Name: 2905, dtype: object\n",
      "\n",
      "Validation :  427 \n",
      "\n",
      "paperId                                                   17145292\n",
      "phenotypeId                                             HP:0003085\n",
      "phenotypeName                                          Long fibula\n",
      "title            Long bone reconstruction with vascularized bon...\n",
      "abstract         The vascularized fibula may be used for long b...\n",
      "Name: 13493, dtype: object\n",
      "paperId                                                   35931175\n",
      "phenotypeId                                             HP:0500011\n",
      "phenotypeName                                          Moon facies\n",
      "title            Iatrogenic Cushing syndrome in 24-hour urine f...\n",
      "abstract         Cushing syndrome (CS) is caused by an excess o...\n",
      "Name: 12113, dtype: object\n",
      "\n",
      "Test :  3847 \n",
      "\n",
      "paperId                                                   19019349\n",
      "phenotypeId                                             HP:0010522\n",
      "phenotypeName                                             Dyslexia\n",
      "title            Elucidating the component processes involved i...\n",
      "abstract         The relationship between rapid automatized nam...\n",
      "Name: 904, dtype: object\n",
      "paperId                                                   16297742\n",
      "phenotypeId                                             HP:0200122\n",
      "phenotypeName                      Atypical or prolonged hepatitis\n",
      "title                                             Viral arthritis.\n",
      "abstract         The role of viruses in the development of acut...\n",
      "Name: 10736, dtype: object\n",
      "\n",
      "Preparing dataloaders...\n",
      "Cleaning abstracts...\n",
      "example: dermatofibrosarcoma protuberans rare malignant slowgrowing locally invasive tumor skin although cases acquired diagnosed adulthood increasing number congenital dermatofibrosarcoma protuberans mimicking benign birthmarks described literature clinical presentation tumor often one indurated exophytic plaque nodule however rare variant present atrophic sclerotic nature report case congenital atrophic dermatofibrosarcoma protuberans groin 7monthold boy successfully treated mohs micrographic surgery\n",
      "Validation\n",
      "Test\n",
      "Preparing loss and evaluator...\n",
      "Loading fitted model...\n"
     ]
    }
   ],
   "source": [
    "from cmath import nan\n",
    "import sentence_transformers\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargar el BERT de partida\n",
    "\n",
    "BERTBASE =  'sentence-transformers/stsb-bert-base'\n",
    "PRITAMDEKAMODEL = 'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb'\n",
    "bertmodel = SentenceTransformer(PRITAMDEKAMODEL)\n",
    "# Se puede aumentar max_seq_length?\n",
    "\n",
    "# Función clean abstract\n",
    "\n",
    "# Download the stopwords from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "cached_stopwords = stopwords.words('english')\n",
    "\n",
    "def clean_abstract(abstract):\n",
    "    if isinstance(abstract, float) and np.isnan(abstract):\n",
    "        return ''\n",
    "    # Convert the text to lowercase\n",
    "    abstract = abstract.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    abstract = abstract.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(abstract)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if not word in cached_stopwords]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    abstract = ' '.join(tokens)\n",
    "\n",
    "    return abstract\n",
    "\n",
    "# Obtener los datos de entrenamiento\n",
    "\n",
    "PATH_DATA = '../pubmed-queries/abstracts'\n",
    "PATH_DATA_CSV = PATH_DATA + '/abstracts.csv'\n",
    "PATH_DATA_FENOTIPOS = '../pubmed-queries/results/phenotypes-22-12-15.csv'\n",
    "PATH_INDEX_FENOTIPOS = PATH_DATA + '/index-phenotypes.csv'\n",
    "SEED = 42\n",
    "\n",
    "dfPapers = pd.read_csv(PATH_DATA_CSV, sep='\\t', low_memory=False, na_values=['', nan])\n",
    "dfPhenotypes = pd.read_csv(PATH_DATA_FENOTIPOS, sep=';', low_memory=False, na_values=['', nan])\n",
    "dfIndex = pd.read_csv(PATH_INDEX_FENOTIPOS, sep='\\t', low_memory=False, na_values=['', nan])\n",
    "\n",
    "# Cargar la ontología\n",
    "\n",
    "from pyhpo import Ontology\n",
    "\n",
    "onto = Ontology('../pubmed-queries/hpo-22-12-15-data')\n",
    "\n",
    "# phenotypeId\tphenotypeName\tnumberPapers\tpaperList\n",
    "\n",
    "# Tomar la lista de fenotipos = tags\n",
    "tags = dfIndex['phenotypeName']\n",
    "numlabels = len(tags)\n",
    "print(numlabels, 'tags')\n",
    "print(tags[:5])\n",
    "\n",
    "from itertools import combinations\n",
    "from sentence_transformers import util\n",
    "\n",
    "# Tomar muestra aleatoria de pares de fenotipos\n",
    "MARGIN = 0.3743\n",
    "if MARGIN == 0: # Estimar un margin apropiado\n",
    "    unique_pairs = combinations(dfIndex['phenotypeName'].drop_duplicates(), 2)\n",
    "    df_pairs = pd.DataFrame(unique_pairs, columns=['phenotype1', 'phenotype2']).sample(frac=0.2, random_state=SEED)\n",
    "    print('Unique pairs:', len(df_pairs))\n",
    "    print('Pair 1:', df_pairs.iloc[0])\n",
    "    #df_pairs['distance']=df_pairs.apply(lambda x: float(losses.SiameseDistanceMetric.COSINE_DISTANCE(torch.from_numpy(model.encode(x['phenotype1'])), torch.from_numpy(model.encode(x['phenotype2'])))), axis=1)\n",
    "    df_pairs['distance'] = df_pairs.apply(lambda x: 1-util.cos_sim(model.encode(x['phenotype1']), model.encode(x['phenotype2'])), axis=1)\n",
    "    margin = min(df_pairs['distance']).numpy()[0][0]\n",
    "    print('Margin:', margin)\n",
    "\n",
    "# Separar abstracts en train, validation y test\n",
    "\n",
    "# quitar NA's en la columna abstract\n",
    "print('Na\\'s:', dfPapers['abstract'].isna().sum())\n",
    "dfPapers = dfPapers.dropna(subset=['abstract'])\n",
    "\n",
    "train = dfPapers.sample(frac=0.1, random_state=SEED)\n",
    "dTest = dfPapers.drop(train.index).sample(frac=0.2, random_state=SEED)\n",
    "dVal = train.sample(frac=0.2, random_state=SEED)\n",
    "dTrain = train.drop(dVal.index)\n",
    "num_examples = len(dTrain)\n",
    "\n",
    "# Considerar train_test_split\n",
    "\n",
    "# paperId\tphenotypeId\tphenotypeName\ttitle\tabstract\n",
    "list_ = [dTrain, dVal, dTest]\n",
    "names = ['Train', 'Validation', 'Test']\n",
    "for j in range(0, 3):\n",
    "    l = list_[j]\n",
    "    print(names[j],': ', len(l), '\\n')\n",
    "    for i in range(0, 2):\n",
    "        print(l.iloc[i])\n",
    "    print('')\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, losses, evaluation, InputExample\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "model = bertmodel\n",
    "\n",
    "mapping = {tag: i for i, tag in enumerate(tags)}\n",
    "\n",
    "\n",
    "def getLabelNumber(phenotypeName):\n",
    "    return mapping[phenotypeName]\n",
    "\n",
    "# TODO: Documentarse cómo se prepara el DataLoader con los pares abstract-fenotipo\n",
    "# imagino que en el conjunto de train solo se usan los abstracts y en el conjunto de validación y test se usan los abstracts y los fenotipos\n",
    "\n",
    "print(\"Preparing dataloaders...\")\n",
    "\n",
    "print('Cleaning abstracts...')\n",
    "print('example:', clean_abstract(dTrain['abstract'].iloc[0]))\n",
    "\n",
    "abstractsTrain = [InputExample(texts=[clean_abstract(x)], label=mapping[y]) for x, y in zip(dTrain['abstract'], dTrain['phenotypeName'])]\n",
    "train_dataloader = DataLoader(abstractsTrain, shuffle=True, batch_size=16)\n",
    "\n",
    "print('Validation')\n",
    "pairsVal = [InputExample(texts=[clean_abstract(x)], label=mapping[y]) for x, y in zip(dTrain['abstract'], dVal['phenotypeName'])]\n",
    "val_dataloader = DataLoader(pairsVal, shuffle=False, batch_size=16)\n",
    "\n",
    "print('Test')\n",
    "pairsTest = [InputExample(texts=[clean_abstract(x)], label=mapping[y]) for x, y in zip(dTrain['abstract'], dTest['phenotypeName'])]\n",
    "test_dataloader = DataLoader(dTest, shuffle=False, batch_size=16)\n",
    "\n",
    "# TODO: Documentarse sobre loss y evaluator\n",
    "\n",
    "print(\"Preparing loss and evaluator...\")\n",
    "soft_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=numlabels)\n",
    "# Esta no sirve porque recibe un par de sentencias y un label, no una sentencia y un label\n",
    "train_loss = losses.BatchAllTripletLoss(model=model, distance_metric=losses.BatchHardTripletLossDistanceFunction.cosine_distance, margin=MARGIN)\n",
    "\n",
    "evaluator = evaluation.LabelAccuracyEvaluator(val_dataloader, '', softmax_model=soft_loss, write_csv=True)\n",
    "\n",
    "\n",
    "# TODO: Documentarse sobre los hiperparámetros y preparar el grid\n",
    "\n",
    "FITTED = True\n",
    "PATH_TUNED = './output/fine-tuned-bio-bert-cosbatch'\n",
    "if FITTED:\n",
    "    print(\"Loading fitted model...\")\n",
    "    fmodel = SentenceTransformer(PATH_TUNED)\n",
    "else:\n",
    "    print(\"Fitting...\")\n",
    "    fmodel = model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        #evaluator=evaluator,\n",
    "        epochs=num_epochs,\n",
    "        #evaluation_steps=4,\n",
    "        warmup_steps=int(0.25*(num_examples//16)),\n",
    "        output_path='./output/fine-tuned-bio-bert',\n",
    "        save_best_model=True,\n",
    "        checkpoint_path='./checkpoint',\n",
    "        checkpoint_save_steps=25,\n",
    "        checkpoint_save_total_limit=5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluar con las deltas\n",
    "Este experimento consiste en comparar las distancias en la ontología de pares de fenotipos arbitrarios con las distancias entre sus embeddings (original y finetuneado). Si las distancias se parecen más que antes en cada epoch quiere decir que el modelo ha aprendido algo.  \n",
    "Algunas consideraciones:\n",
    "* 1. La similitud de Resnik tiene un rango y la distancia coseno otro.\n",
    "* 2. La correlación de Pearson puede ayudar más: queremos que cuando una sea baja la otra también.\n",
    "* 3. Lo ideal sería que exista una isometría entre ambos espacios ($E_1$ la ontología y $E_2\\subset\\mathbb{R}^{768}$ los embeddings):\n",
    "$${\\displaystyle \\exists \\varphi :E_{1}\\to E_{2} \\mid \\forall (x,y)\\in E_{1}\\times E_{1}:\\ d_{1}(x,y)=d_{2}(\\varphi (x),\\varphi (y))}$$\n",
    "y que la isometría sea precisamente el embedding. Pero esto lo veo muy difícil porque un conjunto es el árbol de la ontología y otro una esfera (la similitud coseno no entiende de tamaños), por lo que las distancias se calculan de manera muy distinta.\n",
    "* 4. En cualquier caso este experimento hay que realizarlo igual, porque es la única forma de evaluar el modelo que tenemos sin usar un clasificador.  \n",
    "* 5. Si definimos las siguientes variables aleatorias: $X =$ \"distancia en la ontología entre 2 fenotipos escogidos al azar\"  \n",
    "  $Y_1 = $ \"distancia entre los embeddings obtenidos con el modelo original\"  \n",
    "  $Y_2 = $ \"\"\" finetuneado\".  \n",
    "  Entonces podemos comparar las variables aleatorias $$Z_i = X - Y_i, i = 1,2$$. Si conocemos la distribución de las $Z_{i}$ (ojalá sea normal) podemos hallar un intervalo de confianza al 95% para $Z_i$ y también hacer un contraste para ver si $$\\abs{Z_{2}} < \\abs{Z_{1}}$$ que es lo que queremos comprobar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134571/3005183338.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  lpairs.append([df_emb.iloc[i][0], df_emb.iloc[i][1], df_emb.iloc[i][2], df_emb.iloc[j][0], df_emb.iloc[j][1], df_emb.iloc[j][2]])\n"
     ]
    }
   ],
   "source": [
    "# Pasos de evaluación\n",
    "# 1. Extraer una muestra de fenotipos\n",
    "# 2. Calcular los embeddings de los fenotipos con el modelo original\n",
    "# 3. Calcular los embeddings de los fenotipos con el modelo entrenado\n",
    "# 4. Extraer una muestra de pares de fenotipos con sus embeddings\n",
    "# 5. Para cada muestra calcular\n",
    "    # a. La distancia entre los embeddings (original y tuneado)\n",
    "    # b. La distancia en la ontología\n",
    "# 6. Calcular la correlación entre las distancias a y b\n",
    "# 7. Calcular cuánto se acerca una distancia a otra (los deltas)\n",
    "# 8. Obtener los resultados (csv, gráficos)\n",
    "# 9. Repetir para más epochs\n",
    "\n",
    "# 1. Extraer una muestra de fenotipos\n",
    "# primero probemos con los que tenemos en dfIndex, que son los que se usaron para buscar los abstracts\n",
    "# (validación)\n",
    "# luego se probará con fenotipos no usados (test)\n",
    "\n",
    "dmp = pd.DataFrame(dfIndex, columns=['phenotypeName'])\n",
    "\n",
    "# 2. Calcular los embeddings de los fenotipos con el modelo original\n",
    "# 3. Calcular los embeddings de los fenotipos con el modelo entrenado\n",
    "\n",
    "l1 = bertmodel.encode(dmp[\"phenotypeName\"]) # original\n",
    "l2 = fmodel.encode(dmp[\"phenotypeName\"]) # tuneado\n",
    "\n",
    "emb = zip(dmp[\"phenotypeName\"], l1, l2)\n",
    "\n",
    "# Hasta aquí bien\n",
    "\n",
    "l3 = list(emb)\n",
    "#print('list of embeddings', len(l3))\n",
    "\n",
    "#for i in range(0,len(l3)):\n",
    "    #print(i, l3[i][0])\n",
    "\n",
    "\n",
    "# 3.5: guardar csv con los embeddings\n",
    "df_emb = pd.DataFrame(l3, columns=['phenotypeName', 'original', 'tuned'])\n",
    "df_emb.to_csv(PATH_DATA + '/embeddings.csv', index=False, sep=';')\n",
    "\n",
    "# 4. Extraer una muestra de pares de fenotipos con sus embeddings\n",
    "from itertools import combinations\n",
    "\n",
    "index_pairs = combinations(range(0,len(df_emb)), 2)\n",
    "#print(len(df_emb))\n",
    "#print(len(dmp))\n",
    "#print(index_pairs)\n",
    "lpairs = []\n",
    "for (i,j) in index_pairs:\n",
    "    # extract rows i,j from df_emb\n",
    "    #print(i,j)\n",
    "    #print(df_emb.iloc[i], df_emb.iloc[j])\n",
    "    lpairs.append([df_emb.iloc[i][0], df_emb.iloc[i][1], df_emb.iloc[i][2], df_emb.iloc[j][0], df_emb.iloc[j][1], df_emb.iloc[j][2]])\n",
    "\n",
    "df_pairs = pd.DataFrame(lpairs, columns=['phenotype1', 'original1', 'tuned1', 'phenotype2', 'original2', 'tuned2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Calcular distancias para cada muestra\n",
    "# a. La distancia entre los embeddings (original y tuneado)\n",
    "from math import pi, acos\n",
    "\n",
    "def cos_distance(e1, e2):\n",
    "    sim = util.cos_sim(e1, e2).numpy()[0][0]\n",
    "    dist = 1-acos(sim)/pi\n",
    "    return dist\n",
    "\n",
    "df_pairs['cosOriginal'] = df_pairs.apply(lambda x: (1-util.cos_sim(x['original1'], x['original2']).numpy()[0][0]), axis=1)\n",
    "df_pairs['cosTuned'] = df_pairs.apply(lambda x: (1-util.cos_sim(x['tuned1'], x['tuned2'])).numpy()[0][0], axis=1)\n",
    "\n",
    "df_pairs['cosOriginalTrans'] = df_pairs.apply(lambda x: cos_distance(x['original1'], x['original2']), axis=1)\n",
    "df_pairs['cosTunedTrans'] = df_pairs.apply(lambda x: cos_distance(x['tuned1'], x['tuned2']), axis=1)\n",
    "\n",
    "# b. La distancia en la ontología\n",
    "\n",
    "def onto_distance(name1, name2):\n",
    "    phen1 = onto.get_hpo_object(name1)\n",
    "    phen2 = onto.get_hpo_object(name2)\n",
    "    dist = max(1-phen1.similarity_score(phen2, method='lin'),0)\n",
    "    return dist\n",
    "\n",
    "df_pairs['lin'] = df_pairs.apply(lambda x: onto_distance(x['phenotype1'], x['phenotype2']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Original: 0.037398450879504455\n",
      "MSE Tuned: 0.02057724076595524\n",
      "MSE Original Trans: 0.1822122261928704\n",
      "MSE Tuned Trans: 0.21774972023285336\n",
      "Correlation Original: 0.24957384965635498\n",
      "Correlation Tuned: 0.34942018772031597\n",
      "Correlation Original Trans: -0.25344302519210493\n",
      "Correlation Tuned Trans: -0.3521024953511749\n"
     ]
    }
   ],
   "source": [
    "# 7. Calcular cuánto se acerca una distancia a otra (los deltas)\n",
    "df_pairs['deltaOriginal'] = df_pairs['cosOriginal'] - df_pairs['lin']\n",
    "df_pairs['deltaTuned'] = df_pairs['cosTuned'] - df_pairs['lin']\n",
    "\n",
    "df_pairs['deltaOriginalTrans'] = df_pairs['cosOriginalTrans'] - df_pairs['lin']\n",
    "df_pairs['deltaTunedTrans'] = df_pairs['cosTunedTrans'] - df_pairs['lin']\n",
    "\n",
    "mseOriginal = np.mean(df_pairs['deltaOriginal']**2)\n",
    "mseTuned = np.mean(df_pairs['deltaTuned']**2)\n",
    "mseOriginalTrans = np.mean(df_pairs['deltaOriginalTrans']**2)\n",
    "mseTunedTrans = np.mean(df_pairs['deltaTunedTrans']**2)\n",
    "\n",
    "print('MSE Original:', mseOriginal)\n",
    "print('MSE Tuned:', mseTuned)\n",
    "print('MSE Original Trans:', mseOriginalTrans)\n",
    "print('MSE Tuned Trans:', mseTunedTrans)\n",
    "\n",
    "# 6. Correlación de Pearson\n",
    "\n",
    "corrOriginal = df_pairs['cosOriginal'].corr(df_pairs['lin'])\n",
    "corrTuned = df_pairs['cosTuned'].corr(df_pairs['lin'])\n",
    "corrOriginalTrans = df_pairs['cosOriginalTrans'].corr(df_pairs['lin'])\n",
    "corrTunedTrans = df_pairs['cosTunedTrans'].corr(df_pairs['lin'])\n",
    "\n",
    "print('Correlation Original:', corrOriginal)\n",
    "print('Correlation Tuned:', corrTuned)\n",
    "print('Correlation Original Trans:', corrOriginalTrans)\n",
    "print('Correlation Tuned Trans:', corrTunedTrans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación en el conjunto de Test\n",
    "Pares de fenotipos en general (no solo con los que se ha entrenado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Original: 0.03606777685039624\n",
      "MSE Tuned: 0.02674217008841855\n",
      "Correlation Original: 0.3320614705038265\n",
      "Correlation Tuned: 0.376997982306215\n"
     ]
    }
   ],
   "source": [
    "# 7.5 Evaluación en conjunto de Test\n",
    "#Id;Phenotype;Def\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "L = len(dfPhenotypes)  # Define the upper limit of the range (0 to N)\n",
    "num_samples = 1000  # Number of combinations to sample\n",
    "\n",
    "# Generate all possible combinations of integers from 0 to N\n",
    "all_combinations = list(itertools.combinations(range(0,L), 2))\n",
    "\n",
    "# Randomly sample from the list of all combinations\n",
    "indexTestPairs = random.sample(all_combinations, num_samples)\n",
    "\n",
    "lt1 = []\n",
    "lt2 = []\n",
    "\n",
    "for (i,j) in indexTestPairs:\n",
    "    lt1.append(dfPhenotypes.iloc[i]['Phenotype'])\n",
    "    lt2.append(dfPhenotypes.iloc[j]['Phenotype'])\n",
    "\n",
    "lf1 = fmodel.encode(lt1)\n",
    "lf2 = fmodel.encode(lt2)\n",
    "lo1 = bertmodel.encode(lt1)\n",
    "lo2 = bertmodel.encode(lt2)\n",
    "\n",
    "embs = zip(lt1, lo1, lf1, lt2, lo2, lf2)\n",
    "lpairstest = list(embs)\n",
    "df_pairs_test = pd.DataFrame(lpairstest, columns=['phenotype1', 'original1', 'tuned1', 'phenotype2', 'original2', 'tuned2'])\n",
    "\n",
    "df_pairs_test['cosOriginal'] = df_pairs_test.apply(lambda x: (1-util.cos_sim(x['original1'], x['original2']).numpy()[0][0]), axis=1)\n",
    "df_pairs_test['cosTuned'] = df_pairs_test.apply(lambda x: (1-util.cos_sim(x['tuned1'], x['tuned2'])).numpy()[0][0], axis=1)\n",
    "df_pairs_test['lin'] = df_pairs_test.apply(lambda x: onto_distance(x['phenotype1'], x['phenotype2']), axis=1)\n",
    "\n",
    "# 7. Calcular cuánto se acerca una distancia a otra (los deltas)\n",
    "df_pairs_test['deltaOriginal'] = df_pairs_test['cosOriginal'] - df_pairs_test['lin']\n",
    "df_pairs_test['deltaTuned'] = df_pairs_test['cosTuned'] - df_pairs_test['lin']\n",
    "\n",
    "mseOriginalTest = np.mean(df_pairs_test['deltaOriginal']**2)\n",
    "mseTunedTest = np.mean(df_pairs_test['deltaTuned']**2)\n",
    "\n",
    "print('MSE Original:', mseOriginalTest)\n",
    "print('MSE Tuned:', mseTunedTest)\n",
    "\n",
    "# 6. Correlación de Pearson\n",
    "\n",
    "corrOriginalTest = df_pairs_test['cosOriginal'].corr(df_pairs_test['lin'])\n",
    "corrTunedTest = df_pairs_test['cosTuned'].corr(df_pairs_test['lin'])\n",
    "\n",
    "print('Correlation Original:', corrOriginalTest)\n",
    "print('Correlation Tuned:', corrTunedTest)\n",
    "\n",
    "df_pairs_limpio = df_pairs_test.drop(columns=['original1', 'tuned1', 'original2', 'tuned2'])\n",
    "df_pairs_limpio.to_csv(PATH_DATA + '/test-pairs.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs_limpio = df_pairs.drop(columns=['original1', 'tuned1', 'original2', 'tuned2'])\n",
    "df_pairs_limpio.to_csv(PATH_DATA + '/pairs-deltas.csv', index=False, sep=';')\n",
    "\n",
    "# 8. Obtener los resultados \n",
    "# Índice de fenotipos + Modelo + MSEs -> evaluation.txt\n",
    "\n",
    "with open(PATH_DATA + '/evaluation.txt', 'w') as f:\n",
    "    f.write('Índice de fenotipos: ' + PATH_INDEX_FENOTIPOS + '\\n')\n",
    "    f.write('Modelo original: ' + PRITAMDEKAMODEL + '\\n')\n",
    "    f.write('Modelo tuneado: ' + PATH_TUNED + '\\n')\n",
    "    f.write('Distancias utilizadas: 1-cos_sim(e1,e2), Lin\\n')\n",
    "    f.write('MSE Original: ' + str(mseOriginal) + '\\n')\n",
    "    f.write('MSE Tuned: ' + str(mseTuned) + '\\n')\n",
    "    f.write('Correlación Original: ' + str(corrOriginal) + '\\n')\n",
    "    f.write('Correlación Tuned: ' + str(corrTuned) + '\\n')\n",
    "    f.write('TEST\\n')\n",
    "    f.write('MSE Original: ' + str(mseOriginalTest) + '\\n')\n",
    "    f.write('MSE Tuned: ' + str(mseTunedTest) + '\\n')\n",
    "    f.write('Correlación Original: ' + str(corrOriginalTest) + '\\n')\n",
    "    f.write('Correlación Tuned: ' + str(corrTunedTest) + '\\n')\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
