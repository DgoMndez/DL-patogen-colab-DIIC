{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cargar todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import nan\n",
    "import sentence_transformers\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargar el BERT de partida\n",
    "\n",
    "BERTBASE =  'sentence-transformers/stsb-bert-base'\n",
    "PRITAMDEKAMODEL = 'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb'\n",
    "bertmodel = SentenceTransformer(PRITAMDEKAMODEL)\n",
    "# Se puede aumentar max_seq_length?\n",
    "\n",
    "# Función clean abstract\n",
    "\n",
    "# Download the stopwords from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "cached_stopwords = stopwords.words('english')\n",
    "\n",
    "def clean_abstract(abstract):\n",
    "    if isinstance(abstract, float) and np.isnan(abstract):\n",
    "        return ''\n",
    "    # Convert the text to lowercase\n",
    "    abstract = abstract.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    abstract = abstract.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(abstract)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if not word in cached_stopwords]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    abstract = ' '.join(tokens)\n",
    "\n",
    "    return abstract\n",
    "\n",
    "# Obtener los datos de entrenamiento\n",
    "\n",
    "PATH_DATA = '../pubmed-queries/abstracts'\n",
    "PATH_DATA_CSV = PATH_DATA + '/abstracts.csv'\n",
    "PATH_DATA_FENOTIPOS = '../pubmed-queries/results/phenotypes-22-12-15.csv'\n",
    "PATH_INDEX_FENOTIPOS = PATH_DATA + '/index-phenotypes.csv'\n",
    "SEED = 42\n",
    "\n",
    "dfPapers = pd.read_csv(PATH_DATA_CSV, sep='\\t', low_memory=False, na_values=['', nan])\n",
    "dfPhenotypes = pd.read_csv(PATH_DATA_FENOTIPOS, sep=';', low_memory=False, na_values=['', nan])\n",
    "dfIndex = pd.read_csv(PATH_INDEX_FENOTIPOS, sep='\\t', low_memory=False, na_values=['', nan])\n",
    "\n",
    "# Cargar la ontología\n",
    "\n",
    "from pyhpo import Ontology\n",
    "\n",
    "onto = Ontology('../pubmed-queries/hpo-22-12-15-data')\n",
    "\n",
    "# phenotypeId\tphenotypeName\tnumberPapers\tpaperList\n",
    "\n",
    "# Tomar la lista de fenotipos = tags\n",
    "tags = dfIndex['phenotypeName']\n",
    "numlabels = len(tags)\n",
    "print(numlabels, 'tags')\n",
    "print(tags[:5])\n",
    "# Separar abstracts en train, validation y test\n",
    "\n",
    "# quitar NA's en la columna abstract\n",
    "print('Na\\'s:', dfPapers['abstract'].isna().sum())\n",
    "dfPapers = dfPapers.dropna(subset=['abstract'])\n",
    "\n",
    "train = dfPapers.sample(frac=0.1, random_state=SEED)\n",
    "dTest = dfPapers.drop(train.index).sample(frac=0.2, random_state=SEED)\n",
    "dVal = train.sample(frac=0.2, random_state=SEED)\n",
    "dTrain = train.drop(dVal.index)\n",
    "\n",
    "# Considerar train_test_split\n",
    "\n",
    "# paperId\tphenotypeId\tphenotypeName\ttitle\tabstract\n",
    "list = [dTrain, dVal, dTest]\n",
    "names = ['Train', 'Validation', 'Test']\n",
    "for j in range(0, 3):\n",
    "    l = list[j]\n",
    "    print(names[j],': ', len(l), '\\n')\n",
    "    for i in range(0, 2):\n",
    "        print(l.iloc[i])\n",
    "    print('')\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, losses, evaluation, InputExample\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "model = bertmodel\n",
    "\n",
    "mapping = {tag: i for i, tag in enumerate(tags)}\n",
    "\n",
    "\n",
    "def getLabelNumber(phenotypeName):\n",
    "    return mapping[phenotypeName]\n",
    "\n",
    "# TODO: Documentarse cómo se prepara el DataLoader con los pares abstract-fenotipo\n",
    "# imagino que en el conjunto de train solo se usan los abstracts y en el conjunto de validación y test se usan los abstracts y los fenotipos\n",
    "\n",
    "print(\"Preparing dataloaders...\")\n",
    "\n",
    "print('Cleaning abstracts...')\n",
    "print('example:', clean_abstract(dTrain['abstract'].iloc[0]))\n",
    "\n",
    "abstractsTrain = [InputExample(texts=[clean_abstract(x)], label=mapping[y]) for x, y in zip(dTrain['abstract'], dTrain['phenotypeName'])]\n",
    "train_dataloader = DataLoader(abstractsTrain, shuffle=True, batch_size=16)\n",
    "\n",
    "print('Validation')\n",
    "pairsVal = [InputExample(texts=[clean_abstract(x)], label=mapping[y]) for x, y in zip(dTrain['abstract'], dVal['phenotypeName'])]\n",
    "val_dataloader = DataLoader(pairsVal, shuffle=False, batch_size=16)\n",
    "\n",
    "print('Test')\n",
    "pairsTest = [InputExample(texts=[clean_abstract(x)], label=mapping[y]) for x, y in zip(dTrain['abstract'], dTest['phenotypeName'])]\n",
    "test_dataloader = DataLoader(dTest, shuffle=False, batch_size=16)\n",
    "\n",
    "# TODO: Documentarse sobre loss y evaluator\n",
    "\n",
    "print(\"Preparing loss and evaluator...\")\n",
    "soft_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=numlabels)\n",
    "# Esta no sirve porque recibe un par de sentencias y un label, no una sentencia y un label\n",
    "train_loss = losses.BatchAllTripletLoss(model=model)\n",
    "\n",
    "evaluator = evaluation.LabelAccuracyEvaluator(val_dataloader, '', softmax_model=soft_loss, write_csv=True)\n",
    "\n",
    "\n",
    "# TODO: Documentarse sobre los hiperparámetros y preparar el grid\n",
    "\n",
    "print(\"Fitting...\")\n",
    "FITTED = True\n",
    "PATH_TUNED = './output/fine-tuned-bio-bert'\n",
    "if FITTED:\n",
    "    fmodel = SentenceTransformer(PATH_TUNED)\n",
    "else:\n",
    "    fmodel = model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        #evaluator=evaluator,\n",
    "        epochs=num_epochs,\n",
    "        #evaluation_steps=4,\n",
    "        warmup_steps=int(0.25*(num_examples//16)),\n",
    "        output_path='./output/fine-tuned-bio-bert',\n",
    "        save_best_model=True,\n",
    "        checkpoint_path='./checkpoint',\n",
    "        checkpoint_save_steps=25,\n",
    "        checkpoint_save_total_limit=5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluar con las deltas\n",
    "Este experimento consiste en comparar las distancias en la ontología de pares de fenotipos arbitrarios con las distancias entre sus embeddings. Si las distancias se parecen más que antes en cada epoch quiere decir que el modelo ha aprendido algo.  \n",
    "Una consideración: esta comparación no es una medida exacta de buena representación de los embeddings por los siguientes motivos:\n",
    "* 1. La similitud de Resnik tiene un rango y la distancia coseno otro.\n",
    "* 2. La correlación de Pearson puede ayudar más: queremos que cuando una sea baja la otra también.\n",
    "* 3. Lo ideal sería que exista una isometría entre ambos espacios ($E_1$ la ontología y $E_2\\subset\\mathbb{R}^{768}$ los embeddings):\n",
    "$${\\displaystyle \\exists \\varphi :E_{1}\\to E_{2} \\mid \\forall (x,y)\\in E_{1}\\times E_{1}:\\ d_{1}(x,y)=d_{2}(\\varphi (x),\\varphi (y))}$$\n",
    "y que la isometría sea precisamente el embedding. Pero esto lo veo muy difícil porque un conjunto es el árbol de la ontología y otro una esfera (la similitud coseno no entiende de tamaños), por lo que las distancias se calculan de manera muy distinta.\n",
    "En cualquier caso este experimento hay que realizarlo igual, porque es la única forma de evaluar el modelo que tenemos sin usar un clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasos de evaluación\n",
    "# 1. Extraer una muestra de fenotipos\n",
    "# 2. Calcular los embeddings de los fenotipos con el modelo original\n",
    "# 3. Calcular los embeddings de los fenotipos con el modelo entrenado\n",
    "# 4. Extraer una muestra de pares de fenotipos con sus embeddings\n",
    "# 5. Para cada muestra calcular\n",
    "    # a. La distancia entre los embeddings (original y tuneado)\n",
    "    # b. La distancia en la ontología\n",
    "# 6. Calcular la correlación entre las distancias a y b\n",
    "# 7. Calcular cuánto se acerca una distancia a otra (los deltas)\n",
    "# 8. Obtener los resultados (csv, gráficos)\n",
    "# 9. Repetir para más epochs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
