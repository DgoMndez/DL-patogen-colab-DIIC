{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT 101: Fine tuning\n",
    "* Objetivo: Aprender cómo se hace el fine-tuning de https://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb para la tarea del clasificador gen-fenotipo.  \n",
    "TODO: funcionamiento, dataset, evaluación, entrenamiento.  \n",
    "* Fuentes: https://www.sbert.net/index.html  \n",
    " \n",
    "## 1. Obtener un embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.76342797e-01  2.08358914e-01  5.11056483e-01 -9.67204049e-02\n",
      "  -3.82982075e-01 -7.21406844e-03  9.32689905e-02 -2.83180773e-01\n",
      "   2.75494218e-01 -2.27120504e-01 -1.71401396e-01 -3.90078396e-01\n",
      "  -8.76777530e-01 -1.11652412e-01 -1.34179574e-02 -1.79456607e-01\n",
      "  -2.06889641e-02 -3.93742293e-01  1.02231696e-01  1.07746884e-01\n",
      "   4.20903713e-01  3.29416484e-01  3.86795551e-01  1.77757367e-01\n",
      "  -1.11435950e+00 -6.60615861e-01  8.67752373e-01 -7.14428365e-01\n",
      "   1.35926574e-01  6.41943276e-01 -1.44992888e-01 -2.19857797e-01\n",
      "   9.68627110e-02 -8.06883693e-01 -5.75907826e-01  2.01212510e-01\n",
      "  -2.60848373e-01  4.70776021e-01 -7.82865763e-01 -4.56847250e-01\n",
      "  -3.76819313e-01 -1.85794041e-01 -2.74007201e-01  1.16551824e-01\n",
      "   1.24486730e-01 -7.58684501e-02 -1.05840497e-01 -7.11202389e-03\n",
      "  -4.74651694e-01 -4.52506691e-01 -2.94009358e-01 -4.54989791e-01\n",
      "   5.98688483e-01  1.09791744e+00  9.20965612e-01 -5.48521876e-01\n",
      "  -9.60874185e-02  2.86660165e-01  9.01634276e-01 -9.59560454e-01\n",
      "   5.92173100e-01 -1.17357805e-01  5.83522469e-02  1.03325367e+00\n",
      "  -5.60977817e-01 -9.48747516e-01 -4.34506208e-01  5.01411259e-01\n",
      "  -1.06559038e+00 -2.88558692e-01  1.53313100e-01 -3.60981286e-01\n",
      "  -1.12196684e+00  2.58348942e-01 -3.33216903e-03 -3.89204830e-01\n",
      "   4.32150513e-01  4.27737594e-01  2.52912670e-01  2.34285459e-01\n",
      "   6.85113013e-01 -6.92984045e-01 -5.99742353e-01  1.15968741e-01\n",
      "  -9.39254835e-02  2.27006953e-02  8.19149613e-01  1.07532904e-01\n",
      "   1.66228056e-01  5.35627902e-01 -5.52437566e-02 -1.03413737e+00\n",
      "  -1.90295324e-01 -2.93206900e-01  2.04204485e-01  8.48557115e-01\n",
      "  -5.07711947e-01  4.43150878e-01 -2.62902617e-01  7.21255720e-01\n",
      "   2.87115008e-01 -2.45645598e-01 -6.87213063e-01 -3.39759171e-01\n",
      "   9.26342666e-01 -5.51831961e-01 -7.60714173e-01 -1.80226550e-01\n",
      "  -2.55269349e-01 -9.65503573e-01 -6.72388375e-01 -1.13599725e-01\n",
      "   2.80777998e-02 -9.58039701e-01  2.71330684e-01  1.01467930e-01\n",
      "   1.12930521e-01  1.17882085e+00 -1.46888530e+00  9.59050715e-01\n",
      "   4.82580125e-01  3.32206845e-01 -4.98913169e-01  4.26027894e-01\n",
      "   2.19217330e-01 -1.13797009e+00 -1.26086920e-01  5.97084105e-01\n",
      "  -2.36523658e-01  3.23382676e-01 -4.81090009e-01  1.77382201e-01\n",
      "   5.19116163e-01  6.46112740e-01  4.93577063e-01  1.36767581e-01\n",
      "  -5.37518561e-01  1.56194288e-02 -3.52676988e-01 -1.50406390e-01\n",
      "   4.47320007e-02 -8.71190786e-01  1.18295878e-01 -8.37853789e-01\n",
      "  -2.38617882e-01  1.28592104e-01 -2.70297229e-01  2.33401775e-01\n",
      "  -5.41635931e-01  6.02458775e-01 -3.98826420e-01 -3.79690707e-01\n",
      "  -6.38729215e-01 -8.46217945e-02  6.04618967e-01 -4.09411937e-01\n",
      "  -1.01800948e-01 -6.88290656e-01  1.80027187e-01 -1.70711219e-01\n",
      "   1.59663416e-03  1.12207927e-01  4.16256428e-01  1.11479914e+00\n",
      "  -5.92928350e-01  9.24234331e-01  6.72295868e-01 -9.47570801e-02\n",
      "   9.89350528e-02 -1.11675216e-02  1.05921358e-01 -4.22679603e-01\n",
      "   1.83588378e-02 -6.19892657e-01  4.70768243e-01  7.80977458e-02\n",
      "   1.33781925e-01 -8.36290896e-01  8.29637229e-01 -3.82389069e-01\n",
      "  -8.12156737e-01  1.94213167e-01 -5.68264365e-01 -7.89733648e-01\n",
      "  -2.92557012e-02 -1.49771512e-01 -3.85918170e-01  2.21319601e-01\n",
      "  -5.39584495e-02 -4.68038440e-01  3.89747322e-01 -7.39510823e-03\n",
      "  -1.17805883e-01 -7.89920799e-03  9.06816185e-01 -2.87501991e-01\n",
      "   4.18564737e-01 -1.82877213e-01 -7.65515447e-01 -4.08202857e-01\n",
      "  -1.67083517e-01  3.39933574e-01  6.33495092e-01  9.34716105e-01\n",
      "  -2.10439354e-01  1.29258499e-01  2.46437147e-01  3.09285194e-01\n",
      "  -8.53393912e-01  2.08738893e-01  6.59285933e-02 -9.29015458e-01\n",
      "   1.65213570e-01  5.52107573e-01  1.26517248e+00  1.64298862e-01\n",
      "  -5.70814490e-01 -8.84632170e-02  3.29441071e-01 -5.16087174e-01\n",
      "   2.08845392e-01 -7.18853712e-01  2.07654223e-01  1.55024365e-01\n",
      "   2.82593668e-02  3.26812595e-01 -1.79915094e+00  7.81368375e-01\n",
      "  -1.38975084e-01 -2.88222343e-01  5.10890782e-01  2.34642044e-01\n",
      "   2.72450984e-01  6.39593422e-01  8.07298362e-01 -5.24728537e-01\n",
      "   4.97411668e-01  2.16983289e-01  7.00742364e-01 -1.52696300e+00\n",
      "  -4.30993378e-01  8.22798684e-02  5.14969751e-02 -2.40266308e-01\n",
      "   3.33785236e-01  3.69392067e-01 -8.34425032e-01 -2.63699770e-01\n",
      "   2.52911925e-01  3.71183828e-02  5.73259413e-01 -8.18605945e-02\n",
      "   1.68610573e-01 -7.22647965e-01  9.34445262e-01  3.37219745e-01\n",
      "  -6.63836896e-01  2.20057011e-01 -4.04931575e-01 -3.98449123e-01\n",
      "   3.64224225e-01  4.55320388e-01 -2.89684117e-01  2.41969809e-01\n",
      "   2.19501078e-01 -6.75351262e-01  1.17555118e+00 -2.85117358e-01\n",
      "   4.44711655e-01 -4.31384683e-01  3.95052910e-01  5.15209138e-01\n",
      "  -1.84512362e-01 -3.42267394e-01 -1.78808253e-02  1.11397535e-01\n",
      "   5.61733007e-01  9.02448297e-01 -1.95786417e-01  1.73368782e-01\n",
      "   2.55550385e-01  1.37087200e-02  5.01168787e-01  4.06839103e-01\n",
      "  -3.45378041e-01  5.74700952e-01 -2.19808891e-01 -2.48292506e-01\n",
      "  -4.80863690e-01 -4.84682530e-01 -7.04573691e-01  1.44083425e-01\n",
      "   2.52591163e-01 -1.99232921e-02 -3.32067043e-01 -7.55484641e-01\n",
      "  -4.69668984e-01 -3.93022895e-01 -5.41896462e-01  6.01639450e-01\n",
      "  -2.04976164e-02 -7.19064713e-01 -5.82636595e-01  1.01897025e+00\n",
      "  -3.35486293e-01 -9.92642343e-02  1.31440565e-01 -2.80165792e-01\n",
      "   1.16475081e+00 -6.48483813e-01  2.84669757e-01 -2.10127085e-01\n",
      "  -2.24793583e-01  2.17380933e-02  2.46778786e-01 -4.08180594e-01\n",
      "   3.80871333e-02 -7.22260237e-01  1.75344627e-02 -5.14916718e-01\n",
      "  -1.08827122e-01 -3.04094195e-01 -1.28714576e-01  6.44155145e-01\n",
      "  -2.89941043e-01  1.64633676e-01  5.18014848e-01  2.35028416e-01\n",
      "  -4.51938957e-01  3.23195639e-03  2.02819750e-01 -2.83270389e-01\n",
      "  -1.21828794e-01 -1.11787863e-01  8.17167580e-01  3.61220390e-01\n",
      "  -1.67996168e-01 -1.03164971e+00  3.78548540e-02 -7.66285002e-01\n",
      "  -3.14063191e-01 -2.51337051e-01 -1.16054368e+00 -4.09088910e-01\n",
      "  -2.90689379e-01 -1.01298086e-01 -2.31856167e-01 -5.77152014e-01\n",
      "   3.50265771e-01 -1.93332702e-01 -3.58252823e-01 -8.50905657e-01\n",
      "  -6.59874320e-01  1.78373730e+00  7.62622714e-01  5.38011014e-01\n",
      "   4.24362242e-01  3.50613922e-01  7.00081229e-01  1.05033040e+00\n",
      "   3.41198385e-01 -3.89506131e-01 -7.96463639e-02  3.07807505e-01\n",
      "   2.25011989e-01 -3.59788477e-01  3.21290284e-01  8.66423845e-02\n",
      "  -1.11159801e-01  3.93740326e-01  5.70406497e-01  9.51032434e-03\n",
      "   3.71915162e-01  5.97798288e-01  3.05900835e-02  1.80944353e-02\n",
      "   3.35823894e-01 -2.16131136e-01 -2.72775680e-01 -3.57636958e-01\n",
      "   3.09015721e-01  2.54985482e-01  4.41032678e-01  7.23136812e-02\n",
      "  -4.13517833e-01 -4.63793397e-01  5.80059513e-02  2.57151306e-01\n",
      "   6.55995250e-01 -1.52190953e-01 -3.30848902e-01  5.32068551e-01\n",
      "   2.44565874e-01 -1.80842623e-01 -2.18739137e-01  1.88883971e-02\n",
      "  -5.99865317e-01  8.04420710e-01  4.74585563e-01  1.72318697e-01\n",
      "  -3.07329625e-01  5.30815303e-01 -5.69959879e-01  7.21355109e-03\n",
      "   8.37601721e-01  3.98068607e-01 -1.88098669e-01  5.00042379e-01\n",
      "  -3.68591309e-01 -2.29862794e-01 -2.12706074e-01 -3.01091140e-03\n",
      "   1.39692795e+00  1.10444200e+00  5.73572338e-01 -3.74283195e-01\n",
      "  -1.88249126e-01 -1.28052890e-01  5.62883466e-02  4.27564442e-01\n",
      "  -2.07708180e-01  4.96129632e-01  1.02211213e+00 -1.21115513e-01\n",
      "  -1.41081226e+00  4.42956567e-01 -5.24751544e-01 -4.42382008e-01\n",
      "  -5.64285591e-02  7.07516253e-01 -3.41436356e-01 -5.69270551e-01\n",
      "   1.35548443e-01  3.43705773e-01  4.75635499e-01 -3.30559522e-01\n",
      "   9.24299896e-01  4.40260433e-02  5.65424502e-01  3.84852961e-02\n",
      "  -6.34780943e-01  8.35238397e-01 -2.65315086e-01 -1.84196547e-01\n",
      "  -3.97390991e-01 -7.32318580e-01  3.86175931e-01  8.16053927e-01\n",
      "  -7.41453409e-01 -1.36401266e-01 -1.01600409e-01 -1.47033915e-01\n",
      "  -2.35919341e-01 -2.27115434e-02  3.61291379e-01 -1.73085451e-01\n",
      "   3.35627347e-01 -9.84765470e-01  2.79192567e-01 -3.04852039e-01\n",
      "  -2.48671770e-01  1.10512249e-01 -2.48561636e-01 -4.66603577e-01\n",
      "  -5.78994036e-01 -7.31029749e-01 -2.82835156e-01  4.39303756e-01\n",
      "  -4.78984952e-01 -4.26523536e-01  2.55612254e-01  1.10762969e-01\n",
      "   1.89493686e-01  3.30916226e-01 -9.92040396e-01 -2.39676699e-01\n",
      "  -5.26941419e-01 -1.86770275e-01  7.15965107e-02 -4.77345400e-02\n",
      "  -8.45003843e-01  3.94452512e-01  2.09901243e-01 -6.04108393e-01\n",
      "  -9.62654233e-01 -7.00602591e-01  1.55317694e-01  2.18059167e-01\n",
      "  -6.46349311e-01 -1.00348599e-01 -5.27849197e-01  7.31625110e-02\n",
      "   5.68249039e-02  8.89329493e-01  1.22706562e-01  4.26745377e-02\n",
      "   4.63846236e-01  1.28226122e-02  5.95969595e-02  1.77173674e-01\n",
      "  -3.38848084e-01  9.53716874e-01  7.51039237e-02  2.12849319e-01\n",
      "  -2.21305620e-03  8.12561512e-02  6.98181212e-01 -6.42629266e-01\n",
      "  -5.02820194e-01 -1.36746734e-01 -7.76340246e-01 -7.12156221e-02\n",
      "  -5.19039690e-01 -1.91994160e-01  3.91377509e-01 -1.83065429e-01\n",
      "   1.21175565e-01  2.12368786e-01 -7.19648674e-02 -4.05137599e-01\n",
      "   4.41619873e-01  3.29862058e-01 -1.02396041e-01 -5.80137253e-01\n",
      "   5.10968208e-01 -2.67206393e-02  7.85058677e-01 -4.49200302e-01\n",
      "  -1.87161148e-01  3.04515690e-01  1.42130327e+00  1.14548612e+00\n",
      "   1.01874006e+00  2.57827729e-01 -2.45463371e-01 -3.41513097e-01\n",
      "  -5.71218729e-01  2.12705776e-01 -7.02426374e-01 -2.81959891e-01\n",
      "  -3.90183806e-01 -4.05462891e-01 -1.54944971e-01 -1.16962686e-01\n",
      "   5.37548780e-01  7.58898318e-01  5.09509206e-01 -8.17824751e-02\n",
      "   1.66875467e-01  8.04590106e-01 -1.39710093e+00  1.03254482e-01\n",
      "  -9.03738081e-01 -9.81628656e-01  5.19680083e-01  4.14840072e-01\n",
      "   1.33452877e-01 -3.27753806e+00 -6.26804411e-01  4.24254090e-02\n",
      "   3.05636615e-01  1.22964874e-01 -2.25937217e-01  5.44995219e-02\n",
      "   3.74122918e-01 -2.34316140e-01  3.99991460e-02 -3.03445309e-01\n",
      "   2.73484230e-01 -6.03764594e-01  3.64699841e-01  6.73223495e-01\n",
      "   2.85058677e-01  3.33989501e-01  9.21728909e-02 -1.40169129e-01\n",
      "  -3.80717307e-01 -1.11020908e-01  2.53465623e-01 -2.13717684e-01\n",
      "   2.04804856e-02  1.11036861e+00 -1.96398839e-01  3.74582797e-01\n",
      "   1.50522357e-03  6.64462745e-02 -2.74728179e-01 -8.51088583e-01\n",
      "  -6.82640448e-02  1.75160304e-01  7.35581458e-01 -1.00231791e+00\n",
      "  -5.13996363e-01 -1.42025352e-01  3.25535327e-01 -2.14885265e-01\n",
      "  -7.70936161e-02  2.56165087e-01  5.61468959e-01  6.77204132e-01\n",
      "   1.08018294e-01 -1.51125684e-01  7.56313801e-02 -1.26582384e-01\n",
      "  -1.48025781e-01 -2.24493250e-01  2.33083636e-01 -4.12331879e-01\n",
      "   5.36264539e-01 -2.38260269e-01 -2.50880301e-01 -3.20365936e-01\n",
      "   5.08158088e-01 -3.78232926e-01  3.94597948e-01 -2.29544342e-01\n",
      "  -1.17868936e+00 -7.35997796e-01 -9.74067628e-01  4.54545379e-01\n",
      "   1.45486847e-01 -3.97654325e-01  1.79258898e-01  8.10342848e-01\n",
      "  -1.61347225e-01  8.36134329e-02 -3.93262714e-01  4.50294673e-01\n",
      "  -1.56525910e-01  7.06490576e-01  4.58941400e-01 -2.34867521e-02\n",
      "  -6.10326946e-01 -4.62131239e-02  8.00818298e-03  1.28535438e+00\n",
      "   1.66961834e-01 -1.33947507e-01  3.34704340e-01 -4.04006839e-02\n",
      "   7.15786815e-01 -4.78450567e-01 -1.14824407e-01 -1.38466024e+00\n",
      "  -5.87401055e-02  1.90026611e-01 -1.05254143e-01 -1.27380714e-01\n",
      "   4.83699441e-01 -4.74785507e-01  6.04484081e-01  1.45395339e-01\n",
      "   7.09979609e-02 -2.75397241e-01 -3.84584248e-01  9.14294794e-02\n",
      "  -3.45250964e-01  2.33137995e-01 -2.01870739e-01 -2.38961771e-01\n",
      "  -1.01136796e-01 -1.65837690e-01  2.42145047e-01 -5.87539792e-01\n",
      "   1.25667071e+00  1.73015863e-01  2.41614178e-01 -5.07678628e-01\n",
      "  -7.46813044e-02  2.03934908e-01 -9.57540095e-01 -4.31018770e-01\n",
      "   1.88404292e-01 -6.92968071e-01  3.04414600e-01 -1.02511930e+00\n",
      "   4.83926177e-01 -2.04768986e-01 -1.23929210e-01  4.54083979e-01\n",
      "   3.31734836e-01  2.07283899e-01 -4.79309827e-01  1.63858175e+00\n",
      "   7.99138725e-01  2.99544543e-01  1.09506226e+00  2.32455030e-01\n",
      "   7.66084254e-01  5.91484427e-01  3.83125335e-01  1.78809300e-01\n",
      "   5.26600301e-01 -7.21089602e-01  1.03158486e+00  6.73888624e-01\n",
      "  -5.83173409e-02 -3.10304850e-01  3.61498535e-01 -2.66046703e-01\n",
      "   5.03824390e-02 -1.68981880e-01 -7.14632690e-01 -8.64691064e-02\n",
      "   7.92062223e-01 -2.04481948e-02 -3.77252474e-02  8.04152936e-02\n",
      "  -2.37326890e-01  3.57090652e-01  1.41597584e-01 -1.21202044e-01\n",
      "  -4.62122589e-01  6.85324743e-02 -3.48712027e-01 -3.54540497e-01\n",
      "   3.91506642e-01 -1.03954300e-01 -8.39999169e-02 -5.06167889e-01\n",
      "  -6.41160533e-02  7.88393676e-01  6.74620047e-02 -1.19346894e-01\n",
      "  -1.63377449e-01  3.47540081e-01 -1.41885638e-01 -5.89159966e-01\n",
      "  -6.60207391e-01  2.87188232e-01 -3.33033085e-01 -1.07401118e-01\n",
      "   5.07587314e-01 -8.96724612e-02 -5.49371898e-01  6.25363708e-01\n",
      "  -1.87388100e-02  4.02619034e-01  5.23728281e-02  1.44330338e-01\n",
      "  -6.88484073e-01  3.96171302e-01 -5.84175587e-01  4.66060132e-01\n",
      "   6.66850746e-01 -5.07083297e-01  9.19554159e-02 -2.25882500e-01\n",
      "  -3.51353973e-01 -8.76225978e-02 -4.62622106e-01  2.33131856e-01\n",
      "   1.41329383e-02  7.20486939e-01  7.58927539e-02 -3.59515160e-01\n",
      "  -7.00769424e-01  1.36222839e-02 -9.14274454e-02 -4.94759053e-01\n",
      "  -1.20553501e-01 -4.58713949e-01 -1.09262383e+00  1.98854417e-01\n",
      "   3.57502639e-01  3.87666345e-01  1.51339337e-01  5.89975953e-01]]\n",
      "N tokens= 1\n",
      "Longitud= 768\n",
      "Norm= 14.269959\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "BERTBASE =  'sentence-transformers/stsb-bert-base'\n",
    "PRITAMDEKAMODEL = 'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb'\n",
    "bertmodel = SentenceTransformer(PRITAMDEKAMODEL)\n",
    "model = bertmodel\n",
    "\n",
    "# Sentences we want to encode. Example:\n",
    "sentence = ['This framework generates embeddings for each input sentence']\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "embedding = model.encode(sentence)\n",
    "print(embedding)\n",
    "print(\"N tokens=\", len(embedding))\n",
    "print(\"Longitud=\", len(embedding[0]))\n",
    "print(\"Norm=\", np.linalg.norm(embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos generar el embedding para una lista de frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: [-0.47634262  0.20835875  0.5110564  -0.09672066 -0.38298213 -0.00721423\n",
      "  0.09326907 -0.28318128  0.27549437 -0.22712018] ...\n",
      "Norm= 14.269959\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: [ 0.24558467 -0.38048622  1.1098257  -0.44728062  0.23018073 -0.29509577\n",
      "  0.9107106  -0.13001063 -0.01333597 -0.33322632] ...\n",
      "Norm= 16.170506\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [ 0.43256274 -0.10374534 -0.42705497 -0.5431257   0.22969241 -0.5625215\n",
      " -0.6229514  -1.2054772   1.3298908   0.04342185] ...\n",
      "Norm= 15.025292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our sentences we like to encode\n",
    "sentences = [\n",
    "    \"This framework generates embeddings for each input sentence\",\n",
    "    \"Sentences are passed as a list of string.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "]\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "# Print the embeddings\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding[:10], \"...\")\n",
    "    print(\"Norm=\", np.linalg.norm(embedding))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Comparar embeddings\n",
    "\n",
    "* Similitud coseno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity: tensor([[0.5908]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "emb1 = model.encode(\"This is a red cat with a hat.\")\n",
    "emb2 = model.encode(\"Have you seen my red cat?\")\n",
    "\n",
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro ejemplo para más de 1 pareja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 most similar pairs:\n",
      "A man is riding a horse. \t A man is riding a white horse on an enclosed ground. \t 0.7676\n",
      "A man is eating food. \t A man is eating a piece of bread. \t 0.6427\n",
      "A monkey is playing drums. \t Someone in a gorilla costume is playing a set of drums. \t 0.5967\n",
      "A woman is playing violin. \t A monkey is playing drums. \t 0.3307\n",
      "A woman is playing violin. \t Someone in a gorilla costume is playing a set of drums. \t 0.3277\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating a piece of bread.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "    \"A woman is playing violin.\",\n",
    "    \"Two men pushed carts through the woods.\",\n",
    "    \"A man is riding a white horse on an enclosed ground.\",\n",
    "    \"A monkey is playing drums.\",\n",
    "    \"Someone in a gorilla costume is playing a set of drums.\",\n",
    "]\n",
    "\n",
    "# Encode all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Compute cosine similarity between all pairs\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "# Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim) - 1):\n",
    "    for j in range(i + 1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "# Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top-5 most similar pairs:\")\n",
    "for score, i, j in all_sentence_combinations[0:5]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distancia de Resnik en la ontología: para 2 fenotipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhpo import Ontology\n",
    "import pandas as pd\n",
    "# initilize the Ontology ()\n",
    "onto = Ontology('../pubmed-queries/hpo-22-12-15-data')\n",
    "\n",
    "PATH_DATA_FENOTIPOS = '../pubmed-queries/results/phenotypes-22-12-15.csv'\n",
    "dfPhenotypes = pd.read_csv(PATH_DATA_FENOTIPOS, sep=';', low_memory=False, na_values=[''])\n",
    "\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporomandibular joint ankylosis , Dyslexia :Resnik= 0.00036046861309514645 Lin= 4.885073886735671e-05 Cosine= tensor([[0.1194]])\n",
      "Compensatory scoliosis , Kyphoscoliosis :Resnik= 2.275796718576765 Lin= 1.1389062155173746 Cosine= tensor([[0.7856]])\n"
     ]
    }
   ],
   "source": [
    "# get 2 phenotypes\n",
    "SEED = 42\n",
    "\n",
    "muestraPhenotypes = dfPhenotypes.sample(2, random_state=SEED)\n",
    "\n",
    "pairs = []\n",
    "pheno1 = onto.get_hpo_object(muestraPhenotypes.iloc[0]['Id'])\n",
    "pheno2 = onto.get_hpo_object(muestraPhenotypes.iloc[1]['Id'])\n",
    "pairs.append([pheno1, pheno2])\n",
    "pheno1 = onto.get_hpo_object('Compensatory scoliosis')\n",
    "pheno2 = onto.get_hpo_object('Kyphoscoliosis')\n",
    "pairs.append([pheno1, pheno2])\n",
    "\n",
    "for pheno1, pheno2 in pairs:\n",
    "    # get Resnik similarity\n",
    "    sim = pheno1.similarity_score(pheno2, method='resnik')\n",
    "    sim2 = pheno1.similarity_score(pheno2, method='lin')\n",
    "\n",
    "    # get cosine similarity\n",
    "    embedding1 = model.encode(pheno1.name)\n",
    "    embedding2 = model.encode(pheno2.name)\n",
    "    cosim = util.cos_sim(embedding1, embedding2)\n",
    "\n",
    "    # get names of the phenotypes\n",
    "    print(pheno1.name, \",\", pheno2.name, \":Resnik=\", sim, \"Lin=\", sim2,\"Cosine=\", cosim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenar el modelo\n",
    "* References: https://www.sbert.net/docs/training/overview.html https://www.sbert.net/docs/package_reference/models.html    \n",
    "\n",
    "### Crear una arquitectura\n",
    "Los Sentence Transformers ya vienen como embedding+pooling, así que devuelven un único vector de 768 componentes. El embedding es lo que separa en tokens y devuelve vectores que codifican el texto, el pooling los colapsa en uno solo. Podemos definir nuestra propia arquitectura con distintos embeddings y pooling models. Notar que en el ejemplo el embedding tiene una longitud máxima de secuencia, en el ejemplo final NO DEBERÍA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I do not have my eyes closed\n",
      "Embedding: [ 0.19926079  0.40846884  0.01355856 -0.34057072 -0.01478832 -0.10423365\n",
      "  0.13928898  0.44513947 -0.24763162 -0.49960858] ...\n",
      "Norm= 8.877091\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo: Arquitectura base+pooling\n",
    "from sentence_transformers import models\n",
    "\n",
    "word_embedding_model = models.Transformer(\"bert-base-uncased\", max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "text = \"I do not have my eyes closed\"\n",
    "sentence_embedding = model.encode(text)\n",
    "print(\"Sentence:\", text)\n",
    "print(\"Embedding:\", sentence_embedding[:10], \"...\")\n",
    "print(\"Norm=\", np.linalg.norm(sentence_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En https://www.sbert.net/docs/package_reference/models.html tenemos información concreta de qué módulos hay disponibles. Por ejemplo, para una arquitectura con embeddings del biobert pritamdeka, pooling y normalización pondríamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I do not have my eyes closed\n",
      "Embedding: [-0.01004861  0.04328241  0.00344129  0.02131966 -0.01093566 -0.00743332\n",
      "  0.01978693 -0.01606919 -0.03233068 -0.01528868] ...\n",
      "Norm= 1.0\n"
     ]
    }
   ],
   "source": [
    "normalization_model = models.Normalize()\n",
    "model = bertmodel\n",
    "model.append(normalization_model)\n",
    "\n",
    "sentence_embedding = model.encode(text)\n",
    "print(\"Sentence:\", text)\n",
    "print(\"Embedding:\", sentence_embedding[:10], \"...\")\n",
    "print(\"Norm=\", np.linalg.norm(sentence_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dudas: en principio creía que el modelo que estaba usando JLM estaba normalizado, y por eso comparábamos con la similitud coseno en nuestro planteamiento inicial. Veo que no lo está, entonces es conveniente normalizar el embedding final? Qué efecto tiene hacerlo después del pooling? Y antes también?  \n",
    "### Datos de entrenamiento\n",
    "El ingrediente que falta para entrenar el modelo es la similitud entre 2 frases. Ejemplos de datasets: https://www.sbert.net/examples/training/datasets/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6f1e9d75c142ba8974058d248e538d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b693b10aaf5475cb5d93f608d2c74e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f98d61e3524b56a997f49ee440dde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24d698f0ac1460b992130c3bef324a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferencia:  2.8339291e-05\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import InputExample, losses\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "model = bertmodel\n",
    "model.append(normalization_model)\n",
    "train_examples = [\n",
    "    InputExample(texts=[\"My first sentence\", \"My second sentence\"], label=0.8),\n",
    "    InputExample(texts=[\"Another pair\", \"Unrelated sentence\"], label=0.3),\n",
    "    InputExample(texts=[\"My second sentence\", \"My third sentence\"], label=0.1),\n",
    "]\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "\n",
    "text = \"My third sentence\"\n",
    "embedding1 = model.encode(text)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3)\n",
    "embedding2 = model.encode(text)\n",
    "print(\"Diferencia: \", np.linalg.norm(embedding2-embedding1))\n",
    "# Obtenemos una pequeña diferencia respecto al embedding original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Duda: ¿Cómo se consigue reproducibilidad?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
