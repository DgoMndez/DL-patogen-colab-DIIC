{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning progress: Evaluación del BERT a lo largo de las etapas del finetuning\n",
    "\n",
    "Lo vamos a hacer primero con un conjunto de prueba:\n",
    "* Abstracts de los que se tomó la muestra: [abstracts.csv](../../pubmed-queries/abstracts/abstracts.csv)\n",
    "* Muestra fenotipos/etiquetas: [index-phenotypes.csv](../../pubmed-queries/abstracts/index-phenotypes.csv)\n",
    "* Fenotipos test: [phenotypes-22-12-15.csv](../pubmed-queries/results/phenotypes-22-12-15.csv) = nodos hoja HPO:PhenotypicAbnormality\n",
    "\n",
    "Pasos a seguir:\n",
    "1. Cargar todos los datos:\n",
    "  * BERT de partida.\n",
    "  * Ontología.\n",
    "  * Datos crudos de entrenamiento (abstracts+fenotipos).\n",
    "2. Preparar el entrenamiento:\n",
    "  * Datos procesados de train (dataloaders)\n",
    "  * Función de pérdida: BatchAllTripletLoss\n",
    "  * Función de evaluación:\n",
    "    * a) EmbeddingSimilarityEvaluator\n",
    "      * Preparar pares de fenotipos (train/test?)\n",
    "      * Calcular gold scores\n",
    "    * b) Implementarla (SentenceEvaluator) con la funcionalidad:\n",
    "      * Calcular MSE y correlación Train/Test.\n",
    "      * Escribir datos en un csv.\n",
    "      * Devolver la correlación de Test.\n",
    "3. Fit: probar en el servidor y guardar los resultados.\n",
    "4. Out: mostrar e interpretar los resultados.\n",
    "  * Gráfica MSE / etapa (train/test)\n",
    "  * Gráfica correlación / etapa (train/test)\n",
    "\n",
    "## 1. Cargar todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from cmath import nan\n",
    "import sentence_transformers\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Cargar todos los datos (crudos)\n",
    "from pyhpo import Ontology\n",
    "import os\n",
    "\n",
    "SRCPATH = '../../'\n",
    "SEED = 42\n",
    "\n",
    "# 1.1 BERT de partida\n",
    "\n",
    "PRITAMDEKAMODEL = 'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb'\n",
    "bertmodel = SentenceTransformer(PRITAMDEKAMODEL)\n",
    "\n",
    "# 1.2 Ontología\n",
    "\n",
    "onto = Ontology(SRCPATH+ '/pubmed-queries/hpo-22-12-15-data')\n",
    "\n",
    "# 1.3 Datos crudos de entrenamiento\n",
    "\n",
    "PATH_DATA = SRCPATH + '/pubmed-queries/abstracts'\n",
    "PATH_DATA_CSV = PATH_DATA + '/abstracts.csv'\n",
    "PATH_DATA_FENOTIPOS = SRCPATH + '/pubmed-queries/results/phenotypes-22-12-15.csv'\n",
    "PATH_INDEX_FENOTIPOS = PATH_DATA + '/index-phenotypes.csv'\n",
    "\n",
    "# abstracts\n",
    "dfPapers = pd.read_csv(PATH_DATA_CSV, sep='\\t', low_memory=False, na_values=['', nan])\n",
    "# fenotipos test\n",
    "dfPhenotypes = pd.read_csv(PATH_DATA_FENOTIPOS, sep=';', low_memory=False, na_values=['', nan])\n",
    "# fenotipos train\n",
    "dfIndex = pd.read_csv(PATH_INDEX_FENOTIPOS, sep='\\t', low_memory=False, na_values=['', nan])\n",
    "\n",
    "# Guardar en directorio manejable\n",
    "PATH_TRAINDATA = SRCPATH + '/traindata'\n",
    "\n",
    "# crear directorios si no existen\n",
    "for dir in [PATH_TRAINDATA, PATH_TRAINDATA + '/abstracts',\n",
    "            PATH_TRAINDATA + '/phenotypes', PATH_TRAINDATA + '/onto']:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "dfPapers.to_csv(PATH_TRAINDATA + '/abstracts/abstracts.csv', sep='\\t', index=False)\n",
    "dfPhenotypes.to_csv(PATH_TRAINDATA + '/phenotypes/phenotypes.csv', sep=';', index=False)\n",
    "dfIndex.to_csv(PATH_TRAINDATA + '/phenotypes/index.csv', sep='\\t', index=False)\n",
    "\n",
    "# copiar ontology al directorio tambien\n",
    "os.system('cp -r ' + SRCPATH + '/pubmed-queries/hpo-22-12-15-data ' + PATH_TRAINDATA + '/onto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparar el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Na's: 1854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/domingo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/domingo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean abstracts in:  ../..//traindata/abstracts/abstracts-clean.csv\n",
      "Total abstracts:  23226\n",
      "Number of phenotype tags: 100\n"
     ]
    }
   ],
   "source": [
    "# Procesar los datos de train/loss/test\n",
    "\n",
    "# 2.1. Clean abstracts\n",
    "\n",
    "# Tratar NA's en la columna abstracts -> cambiar por el título\n",
    "def getPhenDesc(phenotypeName):\n",
    "    hpoNode = onto.get_hpo_object(phenotypeName) \n",
    "    description = hpoNode.definition if hpoNode.definition else '\"\"'\n",
    "    return description\n",
    "\n",
    "print('Na\\'s:', dfPapers['abstract'].isna().sum())\n",
    "dfPapers['abstract'] = dfPapers['abstract'].fillna(dfPapers['title'])\n",
    "\n",
    "# Función clean abstract\n",
    "\n",
    "# Download the stopwords from NLTK\n",
    "PATH_NTLK = SRCPATH + '/traindata/nltk'\n",
    "if not os.path.exists(PATH_NTLK):\n",
    "    os.makedirs(PATH_NTLK)\n",
    "\n",
    "os.environ['NLTK_DATA'] = PATH_NTLK\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "cached_stopwords = stopwords.words('english')\n",
    "def clean_abstract(abstract):\n",
    "    if isinstance(abstract, float) and np.isnan(abstract):\n",
    "        return ''\n",
    "    # Convert the text to lowercase\n",
    "    abstract = abstract.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    abstract = abstract.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(abstract)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if not word in cached_stopwords]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    abstract = ' '.join(tokens)\n",
    "\n",
    "    return abstract\n",
    "\n",
    "# Save clean abstracts csv\n",
    "\n",
    "dfPapers['clean_abstract'] = dfPapers['abstract'].apply(clean_abstract)\n",
    "dfPapers.drop(columns=['abstract'], inplace=True)\n",
    "dfPapers.to_csv(PATH_TRAINDATA + '/abstracts/abstracts-clean.csv', sep='\\t', index=False)\n",
    "\n",
    "print('Clean abstracts in: ', PATH_TRAINDATA + '/abstracts/abstracts-clean.csv')\n",
    "print('Total abstracts: ', len(dfPapers))\n",
    "\n",
    "# 2.2 Tags = phenotypes (train)\n",
    "\n",
    "tags = dfIndex['phenotypeName']\n",
    "numlabels = len(tags)\n",
    "print('Number of phenotype tags:', numlabels)\n",
    "mapping = {tag: i for i, tag in enumerate(tags)}\n",
    "\n",
    "def getLabelNumber(phenotypeName):\n",
    "    return mapping[phenotypeName]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Profiling\n",
    "Tomamos solo una muestra del 5% de los abstracts totales para realizar las pruebas más rápido. Cuando funcionen las pruebas se cambia para la versión oficial del experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples: 2323\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, losses, evaluation, InputExample\n",
    "\n",
    "PROFILING = True\n",
    "SAMPLEPERCENT = 0.05 if PROFILING else 1.0\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "train = dfPapers.sample(frac=0.1, random_state=SEED)\n",
    "numexamples = len(train)\n",
    "print('Number of train examples:', numexamples)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
