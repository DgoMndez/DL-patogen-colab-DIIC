{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento finetuning 1ª iteración\n",
    "* Objetivo: Determinar si se consigue aprendizaje con el fine-tuning supervisado\n",
    "* Método: Fine-tuning del tipo (etiqueta=fenotipo, valor=abstract) con una capa softmax al final del BERT\n",
    "* Datos: abstracts.csv, index-phenotypes.csv, phenotypes-22-12-15.csv\n",
    "\n",
    "## 1. Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/domingo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/domingo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from cmath import nan\n",
    "import sentence_transformers\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargar el BERT de partida\n",
    "\n",
    "BERTBASE =  'sentence-transformers/stsb-bert-base'\n",
    "PRITAMDEKAMODEL = 'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb'\n",
    "bertmodel = SentenceTransformer(PRITAMDEKAMODEL)\n",
    "# Se puede aumentar max_seq_length?\n",
    "\n",
    "# Función clean abstract\n",
    "\n",
    "# Download the stopwords from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_abstract(abstract):\n",
    "    # Convert the text to lowercase\n",
    "    abstract = abstract.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    abstract = abstract.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(abstract)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if not word in stopwords.words()]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    abstract = ' '.join(tokens)\n",
    "\n",
    "    return abstract\n",
    "\n",
    "# Obtener los datos de entrenamiento\n",
    "\n",
    "PATH_DATA = '../pubmed-queries/abstracts'\n",
    "PATH_DATA_CSV = PATH_DATA + '/abstracts.csv'\n",
    "PATH_DATA_FENOTIPOS = '../pubmed-queries/results/phenotypes-22-12-15.csv'\n",
    "PATH_INDEX_FENOTIPOS = PATH_DATA + '/index-phenotypes.csv'\n",
    "SEED = 42\n",
    "\n",
    "dfPapers = pd.read_csv(PATH_DATA_CSV, sep='\\t', low_memory=False, na_values=['', nan])\n",
    "dfPhenotypes = pd.read_csv(PATH_DATA_FENOTIPOS, sep=';', low_memory=False, na_values=['', nan])\n",
    "dfIndex = pd.read_csv(PATH_INDEX_FENOTIPOS, sep='\\t', low_memory=False, na_values=['', nan])\n",
    "\n",
    "# Cargar la ontología\n",
    "\n",
    "from pyhpo import Ontology\n",
    "\n",
    "onto = Ontology('../pubmed-queries/hpo-22-12-15-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtener dataset de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tags\n",
      "0                    Temporomandibular joint ankylosis\n",
      "1                                             Dyslexia\n",
      "2    Stippling of the epiphysis of the proximal pha...\n",
      "3                                 Ankle joint effusion\n",
      "4                              Reduced C-peptide level\n",
      "Name: phenotypeName, dtype: object\n",
      "Na's: 1854\n",
      "Train :  1710 \n",
      "\n",
      "paperId                                                   18789087\n",
      "phenotypeId                                             HP:0032217\n",
      "phenotypeName                                     Indurated nodule\n",
      "title            Congenital atrophic dermatofibrosarcoma protub...\n",
      "abstract         Dermatofibrosarcoma protuberans is a rare, mal...\n",
      "Name: 4591, dtype: object\n",
      "paperId                                                   11999861\n",
      "phenotypeId                                             HP:0030553\n",
      "phenotypeName                    Visual acuity no light perception\n",
      "title                            Visual marking and visual change.\n",
      "abstract         Five experiments investigated the types of cha...\n",
      "Name: 2905, dtype: object\n",
      "\n",
      "Validation :  427 \n",
      "\n",
      "paperId                                                   17145292\n",
      "phenotypeId                                             HP:0003085\n",
      "phenotypeName                                          Long fibula\n",
      "title            Long bone reconstruction with vascularized bon...\n",
      "abstract         The vascularized fibula may be used for long b...\n",
      "Name: 13493, dtype: object\n",
      "paperId                                                   35931175\n",
      "phenotypeId                                             HP:0500011\n",
      "phenotypeName                                          Moon facies\n",
      "title            Iatrogenic Cushing syndrome in 24-hour urine f...\n",
      "abstract         Cushing syndrome (CS) is caused by an excess o...\n",
      "Name: 12113, dtype: object\n",
      "\n",
      "Test :  19235 \n",
      "\n",
      "paperId                                                   25483442\n",
      "phenotypeId                                             HP:0012478\n",
      "phenotypeName                    Temporomandibular joint ankylosis\n",
      "title             Management of temporomandibular joint ankylosis.\n",
      "abstract         Temporomandibular joint (TMJ) ankylosis is a p...\n",
      "Name: 0, dtype: object\n",
      "paperId                                                   33213251\n",
      "phenotypeId                                             HP:0012478\n",
      "phenotypeName                    Temporomandibular joint ankylosis\n",
      "title            Pathogenesis of traumatic temporomandibular jo...\n",
      "abstract         OBJECTIVE: To comprehensively review the liter...\n",
      "Name: 1, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# phenotypeId\tphenotypeName\tnumberPapers\tpaperList\n",
    "\n",
    "# Tomar la lista de fenotipos = tags\n",
    "tags = dfIndex['phenotypeName']\n",
    "numlabels = len(tags)\n",
    "print(numlabels, 'tags')\n",
    "print(tags[:5])\n",
    "# Separar abstracts en train, validation y test\n",
    "\n",
    "# quitar NA's en la columna abstract\n",
    "print('Na\\'s:', dfPapers['abstract'].isna().sum())\n",
    "dfPapers = dfPapers.dropna(subset=['abstract'])\n",
    "\n",
    "train = dfPapers.sample(frac=0.1, random_state=SEED)\n",
    "dTest = dfPapers.drop(train.index).sample(frac=0.2, random_state=SEED)\n",
    "dVal = train.sample(frac=0.2, random_state=SEED)\n",
    "dTrain = train.drop(dVal.index)\n",
    "\n",
    "# Considerar train_test_split\n",
    "\n",
    "# paperId\tphenotypeId\tphenotypeName\ttitle\tabstract\n",
    "list = [dTrain, dVal, dTest]\n",
    "names = ['Train', 'Validation', 'Test']\n",
    "for j in range(0, 3):\n",
    "    l = list[j]\n",
    "    print(names[j],': ', len(l), '\\n')\n",
    "    for i in range(0, 2):\n",
    "        print(l.iloc[i])\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ¿Cómo se hace el fine-tuning?\n",
    "Para nuestro caso particular necesitamos pasarle los tags, añadir la red neuronal a la salida y la capa softmax y la forma de evaluación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, losses, evaluation\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "model = bertmodel\n",
    "\n",
    "mapping = {tag: i for i, tag in enumerate(tags)}\n",
    "\n",
    "\n",
    "def getLabelNumber(phenotypeName):\n",
    "    return mapping[phenotypeName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloaders...\n",
      "Cleaning abstracts...\n",
      "example: dermatofibrosarcoma protuberans rare malignant slowgrowing locally invasive tumor skin cases acquired diagnosed adulthood increasing number congenital dermatofibrosarcoma protuberans mimicking benign birthmarks described literature clinical presentation tumor indurated exophytic plaque nodule rare variant present atrophic sclerotic nature report case congenital atrophic dermatofibrosarcoma protuberans groin 7monthold boy successfully treated mohs micrographic surgery\n"
     ]
    }
   ],
   "source": [
    "# TODO: Documentarse cómo se prepara el DataLoader con los pares abstract-fenotipo\n",
    "# imagino que en el conjunto de train solo se usan los abstracts y en el conjunto de validación y test se usan los abstracts y los fenotipos\n",
    "\n",
    "print(\"Preparing dataloaders...\")\n",
    "\n",
    "print('Cleaning abstracts...')\n",
    "print('example:', clean_abstract(dTrain['abstract'].iloc[0]))\n",
    "abstractsTrain = dTrain['abstract']#.apply(clean_abstract)\n",
    "train_dataloader = DataLoader(abstractsTrain, shuffle=True, batch_size=16)\n",
    "\n",
    "print('Validation')\n",
    "pairsVal = dVal[['abstract', 'phenotypeName']]#.apply(lambda x: (x[0], getLabelNumber(x[1])), axis=1)\n",
    "val_dataloader = DataLoader(pairsVal, shuffle=False, batch_size=16)\n",
    "\n",
    "print('Test')\n",
    "pairsTest = dTest[['abstract', 'phenotypeName']]#.apply(lambda x: (x[0], getLabelNumber(x[1])), axis=1)\n",
    "test_dataloader = DataLoader(dTest, shuffle=False, batch_size=16)\n",
    "\n",
    "# TODO: Documentarse sobre loss y evaluator\n",
    "\n",
    "print(\"Preparing loss and evaluator...\")\n",
    "train_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=numlabels)\n",
    "# Esta no sirve porque recibe un par de sentencias y un label, no una sentencia y un label\n",
    "train_loss = losses.BatchAllTripletLoss(model=model)\n",
    "\n",
    "evaluator = evaluation.LabelAccuracyEvaluator(val_dataloader, '', softmax_model=None, write_csv=True)\n",
    "\n",
    "\n",
    "# TODO: Documentarse sobre los hiperparámetros y preparar el grid\n",
    "\n",
    "print(\"Fitting...\")\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    output_path='./output/fine-tuned-bio-bert',\n",
    "    save_best_model=True,\n",
    "    checkpoint_path='./checkpoint',\n",
    "    checkpoint_save_steps=25,\n",
    "    checkpoint_save_total_limit=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
