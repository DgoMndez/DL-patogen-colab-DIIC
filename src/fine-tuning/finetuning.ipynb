{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento finetuning 1ª iteración\n",
    "* Objetivo: Determinar si se consigue aprendizaje con el fine-tuning supervisado\n",
    "* Método: Fine-tuning del tipo (etiqueta=fenotipo, valor=abstract) con una capa softmax al final del BERT\n",
    "* Datos: abstracts.csv, index-phenotypes.csv, phenotypes-22-12-15.csv\n",
    "\n",
    "## 1. Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/domingo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/domingo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from cmath import nan\n",
    "import sentence_transformers\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargar el BERT de partida\n",
    "\n",
    "BERTBASE =  'sentence-transformers/stsb-bert-base'\n",
    "PRITAMDEKAMODEL = 'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb'\n",
    "bertmodel = SentenceTransformer(PRITAMDEKAMODEL)\n",
    "# Se puede aumentar max_seq_length?\n",
    "\n",
    "# Función clean abstract\n",
    "\n",
    "# Download the stopwords from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "cached_stopwords = stopwords.words('english')\n",
    "\n",
    "def clean_abstract(abstract):\n",
    "    if isinstance(abstract, float) and np.isnan(abstract):\n",
    "        return ''\n",
    "    # Convert the text to lowercase\n",
    "    abstract = abstract.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    abstract = abstract.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(abstract)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if not word in cached_stopwords]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    abstract = ' '.join(tokens)\n",
    "\n",
    "    return abstract\n",
    "\n",
    "# Obtener los datos de entrenamiento\n",
    "\n",
    "PATH_DATA = '../pubmed-queries/abstracts'\n",
    "PATH_DATA_CSV = PATH_DATA + '/abstracts.csv'\n",
    "PATH_DATA_FENOTIPOS = '../pubmed-queries/results/phenotypes-22-12-15.csv'\n",
    "PATH_INDEX_FENOTIPOS = PATH_DATA + '/index-phenotypes.csv'\n",
    "SEED = 42\n",
    "\n",
    "dfPapers = pd.read_csv(PATH_DATA_CSV, sep='\\t', low_memory=False, na_values=['', nan])\n",
    "dfPhenotypes = pd.read_csv(PATH_DATA_FENOTIPOS, sep=';', low_memory=False, na_values=['', nan])\n",
    "dfIndex = pd.read_csv(PATH_INDEX_FENOTIPOS, sep='\\t', low_memory=False, na_values=['', nan])\n",
    "\n",
    "# Cargar la ontología\n",
    "\n",
    "from pyhpo import Ontology\n",
    "\n",
    "onto = Ontology('../pubmed-queries/hpo-22-12-15-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtener dataset de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tags\n",
      "0                    Temporomandibular joint ankylosis\n",
      "1                                             Dyslexia\n",
      "2    Stippling of the epiphysis of the proximal pha...\n",
      "3                                 Ankle joint effusion\n",
      "4                              Reduced C-peptide level\n",
      "Name: phenotypeName, dtype: object\n",
      "Na's: 1854\n",
      "Train :  1710 \n",
      "\n",
      "paperId                                                   18789087\n",
      "phenotypeId                                             HP:0032217\n",
      "phenotypeName                                     Indurated nodule\n",
      "title            Congenital atrophic dermatofibrosarcoma protub...\n",
      "abstract         Dermatofibrosarcoma protuberans is a rare, mal...\n",
      "Name: 4591, dtype: object\n",
      "paperId                                                   11999861\n",
      "phenotypeId                                             HP:0030553\n",
      "phenotypeName                    Visual acuity no light perception\n",
      "title                            Visual marking and visual change.\n",
      "abstract         Five experiments investigated the types of cha...\n",
      "Name: 2905, dtype: object\n",
      "\n",
      "Validation :  427 \n",
      "\n",
      "paperId                                                   17145292\n",
      "phenotypeId                                             HP:0003085\n",
      "phenotypeName                                          Long fibula\n",
      "title            Long bone reconstruction with vascularized bon...\n",
      "abstract         The vascularized fibula may be used for long b...\n",
      "Name: 13493, dtype: object\n",
      "paperId                                                   35931175\n",
      "phenotypeId                                             HP:0500011\n",
      "phenotypeName                                          Moon facies\n",
      "title            Iatrogenic Cushing syndrome in 24-hour urine f...\n",
      "abstract         Cushing syndrome (CS) is caused by an excess o...\n",
      "Name: 12113, dtype: object\n",
      "\n",
      "Test :  3847 \n",
      "\n",
      "paperId                                                   19019349\n",
      "phenotypeId                                             HP:0010522\n",
      "phenotypeName                                             Dyslexia\n",
      "title            Elucidating the component processes involved i...\n",
      "abstract         The relationship between rapid automatized nam...\n",
      "Name: 904, dtype: object\n",
      "paperId                                                   16297742\n",
      "phenotypeId                                             HP:0200122\n",
      "phenotypeName                      Atypical or prolonged hepatitis\n",
      "title                                             Viral arthritis.\n",
      "abstract         The role of viruses in the development of acut...\n",
      "Name: 10736, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# phenotypeId\tphenotypeName\tnumberPapers\tpaperList\n",
    "from itertools import combinations\n",
    "\n",
    "# Tomar la lista de fenotipos = tags\n",
    "tags = dfIndex['phenotypeName']\n",
    "numlabels = len(tags)\n",
    "print(numlabels, 'tags')\n",
    "print(tags[:5])\n",
    "# Separar abstracts en train, validation y test\n",
    "\n",
    "# Tomar muestra aleatoria de pares de fenotipos\n",
    "MARGIN = 0.3743\n",
    "if MARGIN == 0: # Estimar un margin apropiado\n",
    "    unique_pairs = combinations(dfIndex['phenotypeName'].drop_duplicates(), 2)\n",
    "    df_pairs = pd.DataFrame(unique_pairs, columns=['phenotype1', 'phenotype2']).sample(frac=0.2, random_state=SEED)\n",
    "    print('Unique pairs:', len(df_pairs))\n",
    "    print('Pair 1:', df_pairs.iloc[0])\n",
    "    #df_pairs['distance']=df_pairs.apply(lambda x: float(losses.SiameseDistanceMetric.COSINE_DISTANCE(torch.from_numpy(model.encode(x['phenotype1'])), torch.from_numpy(model.encode(x['phenotype2'])))), axis=1)\n",
    "    df_pairs['distance'] = df_pairs.apply(lambda x: 1-util.cos_sim(model.encode(x['phenotype1']), model.encode(x['phenotype2'])), axis=1)\n",
    "    margin = min(df_pairs['distance']).numpy()[0][0]\n",
    "    print('Margin:', margin)\n",
    "\n",
    "# quitar NA's en la columna abstract\n",
    "print('Na\\'s:', dfPapers['abstract'].isna().sum())\n",
    "dfPapers = dfPapers.dropna(subset=['abstract'])\n",
    "\n",
    "train = dfPapers.sample(frac=0.1, random_state=SEED)\n",
    "dTest = dfPapers.drop(train.index).sample(frac=0.2, random_state=SEED)\n",
    "dVal = train.sample(frac=0.2, random_state=SEED)\n",
    "dTrain = train.drop(dVal.index)\n",
    "num_examples = len(dTrain)\n",
    "\n",
    "# Considerar train_test_split\n",
    "\n",
    "# paperId\tphenotypeId\tphenotypeName\ttitle\tabstract\n",
    "list = [dTrain, dVal, dTest]\n",
    "names = ['Train', 'Validation', 'Test']\n",
    "for j in range(0, 3):\n",
    "    l = list[j]\n",
    "    print(names[j],': ', len(l), '\\n')\n",
    "    for i in range(0, 2):\n",
    "        print(l.iloc[i])\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ¿Cómo se hace el fine-tuning?\n",
    "Para nuestro caso particular necesitamos pasarle los tags, añadir la red neuronal a la salida y la capa softmax y la forma de evaluación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, losses, evaluation, InputExample\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "model = bertmodel\n",
    "\n",
    "mapping = {tag: i for i, tag in enumerate(tags)}\n",
    "\n",
    "\n",
    "def getLabelNumber(phenotypeName):\n",
    "    return mapping[phenotypeName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloaders...\n",
      "Cleaning abstracts...\n",
      "example: dermatofibrosarcoma protuberans rare malignant slowgrowing locally invasive tumor skin although cases acquired diagnosed adulthood increasing number congenital dermatofibrosarcoma protuberans mimicking benign birthmarks described literature clinical presentation tumor often one indurated exophytic plaque nodule however rare variant present atrophic sclerotic nature report case congenital atrophic dermatofibrosarcoma protuberans groin 7monthold boy successfully treated mohs micrographic surgery\n",
      "Validation\n",
      "Test\n",
      "Preparing loss and evaluator...\n",
      "Fitting...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af86c6acda26443bbddea5d5a008a07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159b46287674489993acb0ead96cb27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Documentarse cómo se prepara el DataLoader con los pares abstract-fenotipo\n",
    "# imagino que en el conjunto de train solo se usan los abstracts y en el conjunto de validación y test se usan los abstracts y los fenotipos\n",
    "\n",
    "print(\"Preparing dataloaders...\")\n",
    "\n",
    "print('Cleaning abstracts...')\n",
    "print('example:', clean_abstract(dTrain['abstract'].iloc[0]))\n",
    "\n",
    "abstractsTrain = [InputExample(texts=[clean_abstract(x)], label=mapping[y]) for x, y in zip(dTrain['abstract'], dTrain['phenotypeName'])]\n",
    "train_dataloader = DataLoader(abstractsTrain, shuffle=True, batch_size=16)\n",
    "\n",
    "print('Validation')\n",
    "pairsVal = [InputExample(texts=[clean_abstract(x)], label=mapping[y]) for x, y in zip(dTrain['abstract'], dVal['phenotypeName'])]\n",
    "val_dataloader = DataLoader(pairsVal, shuffle=False, batch_size=16)\n",
    "\n",
    "print('Test')\n",
    "pairsTest = [InputExample(texts=[clean_abstract(x)], label=mapping[y]) for x, y in zip(dTrain['abstract'], dTest['phenotypeName'])]\n",
    "test_dataloader = DataLoader(dTest, shuffle=False, batch_size=16)\n",
    "\n",
    "# TODO: Documentarse sobre loss y evaluator\n",
    "\n",
    "print(\"Preparing loss and evaluator...\")\n",
    "soft_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=numlabels)\n",
    "# Esta no sirve porque recibe un par de sentencias y un label, no una sentencia y un label\n",
    "train_loss = losses.BatchAllTripletLoss(model=model, distance_metric=losses.BatchHardTripletLossDistanceFunction.cosine_distance, margin=MARGIN)\n",
    "\n",
    "evaluator = evaluation.LabelAccuracyEvaluator(val_dataloader, '', softmax_model=soft_loss, write_csv=True)\n",
    "\n",
    "\n",
    "# TODO: Documentarse sobre los hiperparámetros y preparar el grid\n",
    "\n",
    "print(\"Fitting...\")\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    #evaluator=evaluator,\n",
    "    epochs=num_epochs,\n",
    "    #evaluation_steps=4,\n",
    "    warmup_steps=int(0.25*(num_examples//16)),\n",
    "    output_path='./output/fine-tuned-bio-bert-2',\n",
    "    save_best_model=True,\n",
    "    checkpoint_path='./checkpoint',\n",
    "    checkpoint_save_steps=25,\n",
    "    checkpoint_save_total_limit=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
